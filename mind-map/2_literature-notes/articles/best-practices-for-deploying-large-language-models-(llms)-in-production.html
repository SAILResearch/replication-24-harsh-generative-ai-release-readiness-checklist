<!doctype html><html><head><title>Best Practices for Deploying Large Language Models (LLMs) in Production</title><base href="../../"><meta id="root-path" root-path="../../"><link rel="icon" sizes="96x96" href="https://publish-01.obsidian.md/access/f786db9fac45774fa4f0d8112e232d67/favicon-96x96.png"><meta name="viewport" content="width=device-width,initial-scale=1,user-scalable=yes,minimum-scale=1,maximum-scale=5"><meta charset="UTF-8"><link rel="stylesheet" href="lib/styles/obsidian-styles.css"><link rel="stylesheet" href="lib/styles/theme.css"><link rel="stylesheet" href="lib/styles/plugin-styles.css"><link rel="stylesheet" href="lib/styles/snippets.css"><link rel="stylesheet" href="lib/styles/generated-styles.css"><style>body.css-settings-manager{--heading-spacing:0}</style><script type="module" src="lib/scripts/graph_view.js"></script><script src="lib/scripts/graph_wasm.js"></script><script src="lib/scripts/tinycolor.js"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/pixi.js/7.2.4/pixi.min.js" integrity="sha512-Ch/O6kL8BqUwAfCF7Ie5SX1Hin+BJgYH4pNjRqXdTEqMsis1TUYg+j6nnI9uduPjGaj7DN4UKCZgpvoExt6dkw==" crossorigin="anonymous" referrerpolicy="no-referrer"></script><script src="lib/scripts/webpage.js"></script><script src="lib/scripts/generated.js"></script></head><body class="theme-dark mod-macos native-scrollbars show-inline-title minimal-dracula-dark colorful-active system-shade minimal-dark-black callouts-default trim-cols checkbox-circle pdf-seamless-on pdf-invert-dark pdf-blend-light metadata-heading-off sidebar-tabs-default ribbon-hidden maximize-tables-off tabs-default tab-stack-top minimal-tab-title-hover is-fullscreen loading"><div class="webpage-container"><div class="sidebar-left sidebar"><div class="sidebar-container"><div class="sidebar-sizer"><div class="sidebar-content-positioner"><div class="sidebar-content"><div><label class="theme-toggle-container" for="theme_toggle"><input class="theme-toggle-input" type="checkbox" id="theme_toggle"><div class="toggle-background"></div></label></div><div class="tree-container file-tree mod-nav-indicator" data-depth="0"><div class="tree-header"><span class="sidebar-section-header">obsidian-notes</span><button class="clickable-icon collapse-tree-button is-collapsed"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"></svg></button></div><div class="tree-scroll-area"><div class="tree-item mod-tree-folder mod-collapsible is-collapsed" data-depth="1"><div class="tree-item-contents"><a class="internal-link tree-item-link"><div class="tree-item-icon collapse-icon"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="svg-icon right-triangle"><path d="M3 8L12 17L21 8"></path></svg></div><span class="tree-item-title">2_Literature Notes</span></a></div><div class="tree-item-children" style="display:none"><div class="tree-item mod-tree-folder mod-collapsible is-collapsed" data-depth="2"><div class="tree-item-contents"><a class="internal-link tree-item-link"><div class="tree-item-icon collapse-icon"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="svg-icon right-triangle"><path d="M3 8L12 17L21 8"></path></svg></div><span class="tree-item-title">Articles</span></a></div><div class="tree-item-children" style="display:none"><div class="tree-item mod-tree-file" data-depth="3"><div class="tree-item-contents"><a class="internal-link tree-item-link" href="2_literature-notes/articles/3-things-chatgpt-needs-before-it-can-be-deployed-in-customer-service.html"><span class="tree-item-title">3 Things ChatGPT Needs Before it Can be Deployed in Customer Service</span></a></div><div class="tree-item-children"></div></div><div class="tree-item mod-tree-file" data-depth="3"><div class="tree-item-contents"><a class="internal-link tree-item-link" href="2_literature-notes/articles/a-developer’s-guide-to-llmops-operationalizing-llms-at-scale.html"><span class="tree-item-title">A Developer’s Guide To LLMOps Operationalizing LLMs At Scale</span></a></div><div class="tree-item-children"></div></div><div class="tree-item mod-tree-file" data-depth="3"><div class="tree-item-contents"><a class="internal-link tree-item-link" href="2_literature-notes/articles/age-of-industrialized-ai-dan-jeffries-llms-in-production-conference.html"><span class="tree-item-title">Age of Industrialized AI Dan Jeffries LLMs in Production Conference</span></a></div><div class="tree-item-children"></div></div><div class="tree-item mod-tree-file" data-depth="3"><div class="tree-item-contents"><a class="internal-link tree-item-link" href="2_literature-notes/articles/ai-agents-when-and-how-to-implement.html"><span class="tree-item-title">AI Agents When and How to Implement</span></a></div><div class="tree-item-children"></div></div><div class="tree-item mod-tree-file" data-depth="3"><div class="tree-item-contents"><a class="internal-link tree-item-link" href="2_literature-notes/articles/best-practices-for-deploying-language-models.html"><span class="tree-item-title">Best Practices for Deploying Language Models</span></a></div><div class="tree-item-children"></div></div><div class="tree-item mod-tree-file" data-depth="3"><div class="tree-item-contents"><a class="internal-link tree-item-link" href="2_literature-notes/articles/best-practices-for-deploying-large-language-models-(llms)-in-production.html"><span class="tree-item-title">Best Practices for Deploying Large Language Models (LLMs) in Production</span></a></div><div class="tree-item-children"></div></div><div class="tree-item mod-tree-file" data-depth="3"><div class="tree-item-contents"><a class="internal-link tree-item-link" href="2_literature-notes/articles/best-practices-for-large-language-model-(llm)-deployment.html"><span class="tree-item-title">Best Practices for Large Language Model (LLM) Deployment</span></a></div><div class="tree-item-children"></div></div><div class="tree-item mod-tree-file" data-depth="3"><div class="tree-item-contents"><a class="internal-link tree-item-link" href="2_literature-notes/articles/best-practices-for-monitoring-large-language-models.html"><span class="tree-item-title">Best Practices for Monitoring Large Language Models</span></a></div><div class="tree-item-children"></div></div><div class="tree-item mod-tree-file" data-depth="3"><div class="tree-item-contents"><a class="internal-link tree-item-link" href="2_literature-notes/articles/building-llm-applications-for-production-chip-huyen-llms-in-prod-conference.html"><span class="tree-item-title">Building LLM Applications for Production Chip Huyen LLMs in Prod Conference</span></a></div><div class="tree-item-children"></div></div><div class="tree-item mod-tree-file" data-depth="3"><div class="tree-item-contents"><a class="internal-link tree-item-link" href="2_literature-notes/articles/comply-or-die-the-rise-of-the-ai-governance-stack.html"><span class="tree-item-title">Comply or Die The Rise of the AI Governance Stack</span></a></div><div class="tree-item-children"></div></div><div class="tree-item mod-tree-file" data-depth="3"><div class="tree-item-contents"><a class="internal-link tree-item-link" href="2_literature-notes/articles/concepts-for-reliability-of-llms-in-production.html"><span class="tree-item-title">Concepts for Reliability of LLMs in Production</span></a></div><div class="tree-item-children"></div></div><div class="tree-item mod-tree-file" data-depth="3"><div class="tree-item-contents"><a class="internal-link tree-item-link" href="2_literature-notes/articles/controlled-and-compliant-ai-applications-daniel-whitenack-llms-in-production-conference-part-2.html"><span class="tree-item-title">Controlled and Compliant AI Applications Daniel Whitenack LLMs in Production Conference Part 2</span></a></div><div class="tree-item-children"></div></div><div class="tree-item mod-tree-file" data-depth="3"><div class="tree-item-contents"><a class="internal-link tree-item-link" href="2_literature-notes/articles/deploying-large-language-models-in-production-llm-deployment-challenges.html"><span class="tree-item-title">Deploying Large Language Models in Production LLM Deployment Challenges</span></a></div><div class="tree-item-children"></div></div><div class="tree-item mod-tree-file" data-depth="3"><div class="tree-item-contents"><a class="internal-link tree-item-link" href="2_literature-notes/articles/deploying-large-language-models-in-production-orchestrating-llms.html"><span class="tree-item-title">Deploying Large Language Models in Production Orchestrating LLMs</span></a></div><div class="tree-item-children"></div></div><div class="tree-item mod-tree-file" data-depth="3"><div class="tree-item-contents"><a class="internal-link tree-item-link" href="2_literature-notes/articles/deploying-large-language-models-in-production-the-anatomy-of-llm-applications.html"><span class="tree-item-title">Deploying Large Language Models in Production The Anatomy of LLM Applications</span></a></div><div class="tree-item-children"></div></div><div class="tree-item mod-tree-file" data-depth="3"><div class="tree-item-contents"><a class="internal-link tree-item-link" href="2_literature-notes/articles/design-patterns-for-llm-systems-&amp;-products.html"><span class="tree-item-title">Design Patterns for LLM Systems &amp; Products</span></a></div><div class="tree-item-children"></div></div><div class="tree-item mod-tree-file" data-depth="3"><div class="tree-item-contents"><a class="internal-link tree-item-link" href="2_literature-notes/articles/devtools-for-language-models-—-predicting-the-future.html"><span class="tree-item-title">DevTools for Language Models — Predicting the Future</span></a></div><div class="tree-item-children"></div></div><div class="tree-item mod-tree-file" data-depth="3"><div class="tree-item-contents"><a class="internal-link tree-item-link" href="2_literature-notes/articles/edition-21-a-framework-to-securely-use-llms-in-companies-part-1-overview-of-risks.html"><span class="tree-item-title">Edition 21 A Framework to Securely Use LLMs in Companies - Part 1 Overview of Risks</span></a></div><div class="tree-item-children"></div></div><div class="tree-item mod-tree-file" data-depth="3"><div class="tree-item-contents"><a class="internal-link tree-item-link" href="2_literature-notes/articles/edition-22-a-framework-to-securely-use-llms-in-companies-part-2-managing-risk.html"><span class="tree-item-title">Edition 22 A Framework to Securely Use LLMs in Companies - Part 2 Managing Risk</span></a></div><div class="tree-item-children"></div></div><div class="tree-item mod-tree-file" data-depth="3"><div class="tree-item-contents"><a class="internal-link tree-item-link" href="2_literature-notes/articles/emerging-architectures-for-llm-applications.html"><span class="tree-item-title">Emerging Architectures for LLM Applications</span></a></div><div class="tree-item-children"></div></div><div class="tree-item mod-tree-file" data-depth="3"><div class="tree-item-contents"><a class="internal-link tree-item-link" href="2_literature-notes/articles/emerging-architectures-for-llms-applications-datasciencedojo.html"><span class="tree-item-title">Emerging Architectures for LLMs Applications - datasciencedojo</span></a></div><div class="tree-item-children"></div></div><div class="tree-item mod-tree-file" data-depth="3"><div class="tree-item-contents"><a class="internal-link tree-item-link" href="2_literature-notes/articles/emerging-patterns-for-llms-in-production-willem-pienaar-llms-in-prod-conference-lightning-talk.html"><span class="tree-item-title">Emerging Patterns for LLMs in Production Willem Pienaar LLMs in Prod Conference Lightning Talk</span></a></div><div class="tree-item-children"></div></div><div class="tree-item mod-tree-file" data-depth="3"><div class="tree-item-contents"><a class="internal-link tree-item-link" href="2_literature-notes/articles/ensuring-accuracy-and-quality-in-llm-driven-products-adam-nolte-llms-in-prod-conference.html"><span class="tree-item-title">Ensuring Accuracy and Quality in LLM-driven Products Adam Nolte LLMs in Prod Conference</span></a></div><div class="tree-item-children"></div></div><div class="tree-item mod-tree-file" data-depth="3"><div class="tree-item-contents"><a class="internal-link tree-item-link" href="2_literature-notes/articles/evaluating-rag-pipelines-with-ragas-+-langsmith.html"><span class="tree-item-title">Evaluating RAG Pipelines With Ragas + LangSmith</span></a></div><div class="tree-item-children"></div></div><div class="tree-item mod-tree-file" data-depth="3"><div class="tree-item-contents"><a class="internal-link tree-item-link" href="2_literature-notes/articles/exploring-llm-apps-the-langchain-paradigm-and-future-alternatives.html"><span class="tree-item-title">Exploring LLM Apps The LangChain Paradigm and Future Alternatives</span></a></div><div class="tree-item-children"></div></div><div class="tree-item mod-tree-file" data-depth="3"><div class="tree-item-contents"><a class="internal-link tree-item-link" href="2_literature-notes/articles/finetuning-large-language-models.html"><span class="tree-item-title">Finetuning Large Language Models</span></a></div><div class="tree-item-children"></div></div><div class="tree-item mod-tree-file" data-depth="3"><div class="tree-item-contents"><a class="internal-link tree-item-link" href="2_literature-notes/articles/fmopsllmops-operationalize-generative-ai-and-differences-with-mlops.html"><span class="tree-item-title">FMOpsLLMOps Operationalize Generative AI and Differences With MLOps</span></a></div><div class="tree-item-children"></div></div><div class="tree-item mod-tree-file" data-depth="3"><div class="tree-item-contents"><a class="internal-link tree-item-link" href="2_literature-notes/articles/four-ways-that-enterprises-deploy-llms.html"><span class="tree-item-title">Four Ways That Enterprises Deploy LLMs</span></a></div><div class="tree-item-children"></div></div><div class="tree-item mod-tree-file" data-depth="3"><div class="tree-item-contents"><a class="internal-link tree-item-link" href="2_literature-notes/articles/from-concept-to-practice-learnings-from-llms-for-enterprise-production-–-part-0.html"><span class="tree-item-title">From Concept to Practice Learnings From LLMs for Enterprise Production – Part 0</span></a></div><div class="tree-item-children"></div></div><div class="tree-item mod-tree-file" data-depth="3"><div class="tree-item-contents"><a class="internal-link tree-item-link" href="2_literature-notes/articles/from-out-of-the-box-to-tailor-made-developing-and-deploying-enterprise-generative-ai-tools.html"><span class="tree-item-title">From Out-of-the-Box to Tailor-Made Developing and Deploying Enterprise Generative AI Tools</span></a></div><div class="tree-item-children"></div></div><div class="tree-item mod-tree-file" data-depth="3"><div class="tree-item-contents"><a class="internal-link tree-item-link" href="2_literature-notes/articles/generative-ai-a-creative-new-world.html"><span class="tree-item-title">Generative AI A Creative New World</span></a></div><div class="tree-item-children"></div></div><div class="tree-item mod-tree-file" data-depth="3"><div class="tree-item-contents"><a class="internal-link tree-item-link" href="2_literature-notes/articles/generative-ai-is-exploding-these-are-the-most-important-trends-to-know.html"><span class="tree-item-title">Generative AI Is Exploding. These Are the Most Important Trends to Know</span></a></div><div class="tree-item-children"></div></div><div class="tree-item mod-tree-file" data-depth="3"><div class="tree-item-contents"><a class="internal-link tree-item-link" href="2_literature-notes/articles/generative-ai’s-act-two.html"><span class="tree-item-title">Generative AI’s Act Two</span></a></div><div class="tree-item-children"></div></div><div class="tree-item mod-tree-file" data-depth="3"><div class="tree-item-contents"><a class="internal-link tree-item-link" href="2_literature-notes/articles/guardrails-for-llms-a-practical-approach-shreya-rajpal-llms-in-prod-conference-part-2.html"><span class="tree-item-title">Guardrails for LLMs A Practical Approach Shreya Rajpal LLMs in Prod Conference Part 2</span></a></div><div class="tree-item-children"></div></div><div class="tree-item mod-tree-file" data-depth="3"><div class="tree-item-contents"><a class="internal-link tree-item-link" href="2_literature-notes/articles/guardrails-what-are-they-and-how-can-you-use-nemo-and-guardrails-ai-to-safeguard-llms.html"><span class="tree-item-title">Guardrails What Are They and How Can You Use NeMo and Guardrails AI to Safeguard LLMs</span></a></div><div class="tree-item-children"></div></div><div class="tree-item mod-tree-file" data-depth="3"><div class="tree-item-contents"><a class="internal-link tree-item-link" href="2_literature-notes/articles/guiding-llms-while-staying-in-the-driver's-seat-jacob-van-gogh-llms-in-prod-con-lightning-talk.html"><span class="tree-item-title">Guiding LLMs While Staying in the Driver's Seat Jacob Van Gogh LLMs in Prod Con Lightning Talk</span></a></div><div class="tree-item-children"></div></div><div class="tree-item mod-tree-file" data-depth="3"><div class="tree-item-contents"><a class="internal-link tree-item-link" href="2_literature-notes/articles/harry-browne’s-17-golden-rules-of-financial-safety.html"><span class="tree-item-title">Harry Browne’s 17 Golden Rules of Financial Safety</span></a></div><div class="tree-item-children"></div></div><div class="tree-item mod-tree-file" data-depth="3"><div class="tree-item-contents"><a class="internal-link tree-item-link" href="2_literature-notes/articles/how-ray-solves-common-production-challenges-for-generative-ai-infrastructure.html"><span class="tree-item-title">How Ray Solves Common Production Challenges for Generative AI Infrastructure</span></a></div><div class="tree-item-children"></div></div><div class="tree-item mod-tree-file" data-depth="3"><div class="tree-item-contents"><a class="internal-link tree-item-link" href="2_literature-notes/articles/how-to-evaluate-your-llm-applications.html"><span class="tree-item-title">How to Evaluate Your LLM Applications</span></a></div><div class="tree-item-children"></div></div><div class="tree-item mod-tree-file" data-depth="3"><div class="tree-item-contents"><a class="internal-link tree-item-link" href="2_literature-notes/articles/illustrating-reinforcement-learning-from-human-feedback.html"><span class="tree-item-title">Illustrating Reinforcement Learning From Human Feedback</span></a></div><div class="tree-item-children"></div></div><div class="tree-item mod-tree-file" data-depth="3"><div class="tree-item-contents"><a class="internal-link tree-item-link" href="2_literature-notes/articles/improving-llms-in-production-with-observability.html"><span class="tree-item-title">Improving LLMs in Production With Observability</span></a></div><div class="tree-item-children"></div></div><div class="tree-item mod-tree-file" data-depth="3"><div class="tree-item-contents"><a class="internal-link tree-item-link" href="2_literature-notes/articles/introduction-to-retrieval-augmented-generation.html"><span class="tree-item-title">Introduction to Retrieval Augmented Generation</span></a></div><div class="tree-item-children"></div></div><div class="tree-item mod-tree-file" data-depth="3"><div class="tree-item-contents"><a class="internal-link tree-item-link" href="2_literature-notes/articles/llm-deployment-with-nlp-models-meryem-arik-llms-in-production-conference-lightning-talk-2.html"><span class="tree-item-title">LLM Deployment With NLP Models Meryem Arik LLMs in Production Conference Lightning Talk 2</span></a></div><div class="tree-item-children"></div></div><div class="tree-item mod-tree-file" data-depth="3"><div class="tree-item-contents"><a class="internal-link tree-item-link" href="2_literature-notes/articles/llm-evaluation-assessing-large-language-models-using-their-peers.html"><span class="tree-item-title">LLM Evaluation Assessing Large Language Models Using Their Peers</span></a></div><div class="tree-item-children"></div></div><div class="tree-item mod-tree-file" data-depth="3"><div class="tree-item-contents"><a class="internal-link tree-item-link" href="2_literature-notes/articles/llm-observability-one-small-step-for-spans,-one-giant-leap-for-span-kinds.html"><span class="tree-item-title">LLM Observability One Small Step for Spans, One Giant Leap for Span-Kinds</span></a></div><div class="tree-item-children"></div></div><div class="tree-item mod-tree-file" data-depth="3"><div class="tree-item-contents"><a class="internal-link tree-item-link" href="2_literature-notes/articles/llmops-the-future-of-mlops-for-generative-ai.html"><span class="tree-item-title">LLMOps The Future of MLOps for Generative AI</span></a></div><div class="tree-item-children"></div></div><div class="tree-item mod-tree-file" data-depth="3"><div class="tree-item-contents"><a class="internal-link tree-item-link" href="2_literature-notes/articles/ml's-hidden-tasks-a-checklist-for-developers-when-building-ml-systems.html"><span class="tree-item-title">ML's Hidden Tasks A Checklist for Developers When Building ML Systems</span></a></div><div class="tree-item-children"></div></div><div class="tree-item mod-tree-file" data-depth="3"><div class="tree-item-contents"><a class="internal-link tree-item-link" href="2_literature-notes/articles/monitoring-llms-metrics,-challenges,-&amp;-hallucinations.html"><span class="tree-item-title">Monitoring LLMs Metrics, Challenges, &amp; Hallucinations</span></a></div><div class="tree-item-children"></div></div><div class="tree-item mod-tree-file" data-depth="3"><div class="tree-item-contents"><a class="internal-link tree-item-link" href="2_literature-notes/articles/no-rose-without-a-thorn-obstacles-to-successful-llm-deployments-tanmay-chopra-llms-in-prod.html"><span class="tree-item-title">No Rose Without a Thorn - Obstacles to Successful LLM Deployments Tanmay Chopra LLMs in Prod</span></a></div><div class="tree-item-children"></div></div><div class="tree-item mod-tree-file" data-depth="3"><div class="tree-item-contents"><a class="internal-link tree-item-link" href="2_literature-notes/articles/nvidia-enables-trustworthy,-safe,-and-secure-large-language-model-conversational-systems.html"><span class="tree-item-title">NVIDIA Enables Trustworthy, Safe, and Secure Large Language Model Conversational Systems</span></a></div><div class="tree-item-children"></div></div><div class="tree-item mod-tree-file" data-depth="3"><div class="tree-item-contents"><a class="internal-link tree-item-link" href="2_literature-notes/articles/production-deployment-checklist-for-machine-learning-models.html"><span class="tree-item-title">Production Deployment Checklist for Machine Learning Models</span></a></div><div class="tree-item-children"></div></div><div class="tree-item mod-tree-file" data-depth="3"><div class="tree-item-contents"><a class="internal-link tree-item-link" href="2_literature-notes/articles/security-challenges-in-llm-adoption-for-enterprises-and-how-to-solve-them.html"><span class="tree-item-title">Security Challenges in LLM Adoption for Enterprises and How to Solve Them</span></a></div><div class="tree-item-children"></div></div><div class="tree-item mod-tree-file" data-depth="3"><div class="tree-item-contents"><a class="internal-link tree-item-link" href="2_literature-notes/articles/the-confidence-checklist-for-llms-in-production-rohit-agarwal-llms-in-prod-conference-part-2.html"><span class="tree-item-title">The Confidence Checklist for LLMs in Production Rohit Agarwal LLMs in Prod Conference Part 2</span></a></div><div class="tree-item-children"></div></div><div class="tree-item mod-tree-file" data-depth="3"><div class="tree-item-contents"><a class="internal-link tree-item-link" href="2_literature-notes/articles/the-generative-ai-life-cycle.html"><span class="tree-item-title">The Generative AI Life-Cycle</span></a></div><div class="tree-item-children"></div></div><div class="tree-item mod-tree-file" data-depth="3"><div class="tree-item-contents"><a class="internal-link tree-item-link" href="2_literature-notes/articles/the-gradient-of-generative-ai-release-methods-and-considerations.html"><span class="tree-item-title">The Gradient of Generative AI Release Methods and Considerations</span></a></div><div class="tree-item-children"></div></div><div class="tree-item mod-tree-file" data-depth="3"><div class="tree-item-contents"><a class="internal-link tree-item-link" href="2_literature-notes/articles/the-new-language-model-stack.html"><span class="tree-item-title">The New Language Model Stack</span></a></div><div class="tree-item-children"></div></div><div class="tree-item mod-tree-file" data-depth="3"><div class="tree-item-contents"><a class="internal-link tree-item-link" href="2_literature-notes/articles/the_practical_guide_deploying_large_language_models.html"><span class="tree-item-title">The_Practical_Guide_Deploying_Large_Language_Models</span></a></div><div class="tree-item-children"></div></div><div class="tree-item mod-tree-file" data-depth="3"><div class="tree-item-contents"><a class="internal-link tree-item-link" href="2_literature-notes/articles/understanding-llmops-large-language-model-operations.html"><span class="tree-item-title">Understanding LLMOps Large Language Model Operations</span></a></div><div class="tree-item-children"></div></div></div></div><div class="tree-item mod-tree-file" data-depth="2"><div class="tree-item-contents"><a class="internal-link tree-item-link" href="2_literature-notes/glr-–-references.html"><span class="tree-item-title">GLR – References</span></a></div><div class="tree-item-children"></div></div></div></div><div class="tree-item mod-tree-file" data-depth="1"><div class="tree-item-contents"><a class="internal-link tree-item-link" href="mind-map-–-generative-ai-release-checklist-3.html"><span class="tree-item-title">Mind Map – Generative AI Release Checklist 3</span></a></div><div class="tree-item-children"></div></div></div></div></div></div></div></div><div class="sidebar-gutter"><div class="clickable-icon sidebar-collapse-icon"><svg xmlns="http://www.w3.org/2000/svg" width="100%" height="100%" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="3" stroke-linecap="round" stroke-linejoin="round" class="svg-icon"><path d="M21 3H3C1.89543 3 1 3.89543 1 5V19C1 20.1046 1.89543 21 3 21H21C22.1046 21 23 20.1046 23 19V5C23 3.89543 22.1046 3 21 3Z"></path><path d="M10 4V20"></path><path d="M4 7H7"></path><path d="M4 10H7"></path><path d="M4 13H7"></path></svg></div></div></div><div class="document-container show"><div class="markdown-preview-view markdown-rendered node-insert-event is-readable-line-width allow-fold-headings show-indentation-guide allow-fold-lists show-properties" style="tab-size:4"><style id="MJX-CHTML-styles">mjx-container[jax=CHTML]{line-height:0}mjx-container [space="1"]{margin-left:.111em}mjx-container [space="2"]{margin-left:.167em}mjx-container [space="3"]{margin-left:.222em}mjx-container [space="4"]{margin-left:.278em}mjx-container [space="5"]{margin-left:.333em}mjx-container [rspace="1"]{margin-right:.111em}mjx-container [rspace="2"]{margin-right:.167em}mjx-container [rspace="3"]{margin-right:.222em}mjx-container [rspace="4"]{margin-right:.278em}mjx-container [rspace="5"]{margin-right:.333em}mjx-container [size="s"]{font-size:70.7%}mjx-container [size=ss]{font-size:50%}mjx-container [size=Tn]{font-size:60%}mjx-container [size=sm]{font-size:85%}mjx-container [size=lg]{font-size:120%}mjx-container [size=Lg]{font-size:144%}mjx-container [size=LG]{font-size:173%}mjx-container [size=hg]{font-size:207%}mjx-container [size=HG]{font-size:249%}mjx-container [width=full]{width:100%}mjx-box{display:inline-block}mjx-block{display:block}mjx-itable{display:inline-table}mjx-row{display:table-row}mjx-row>*{display:table-cell}mjx-mtext{display:inline-block}mjx-mstyle{display:inline-block}mjx-merror{display:inline-block;color:red;background-color:#ff0}mjx-mphantom{visibility:hidden}mjx-assistive-mml{top:0;left:0;clip:rect(1px,1px,1px,1px);user-select:none;position:absolute!important;padding:1px 0 0!important;border:0!important;display:block!important;width:auto!important;overflow:hidden!important}mjx-assistive-mml[display=block]{width:100%!important}mjx-math{display:inline-block;text-align:left;line-height:0;text-indent:0;font-style:normal;font-weight:400;font-size:100%;letter-spacing:normal;border-collapse:collapse;overflow-wrap:normal;word-spacing:normal;white-space:nowrap;direction:ltr;padding:1px 0}mjx-container[jax=CHTML][display=true]{display:block;text-align:center;margin:1em 0}mjx-container[jax=CHTML][display=true][width=full]{display:flex}mjx-container[jax=CHTML][display=true] mjx-math{padding:0}mjx-container[jax=CHTML][justify=left]{text-align:left}mjx-container[jax=CHTML][justify=right]{text-align:right}mjx-mo{display:inline-block;text-align:left}mjx-stretchy-h{display:inline-table;width:100%}mjx-stretchy-h>*{display:table-cell;width:0}mjx-stretchy-h>*>mjx-c{display:inline-block;transform:scaleX(1)}mjx-stretchy-h>*>mjx-c::before{display:inline-block;width:initial}mjx-stretchy-h>mjx-ext{overflow:clip visible;width:100%}mjx-stretchy-h>mjx-ext>mjx-c::before{transform:scaleX(500)}mjx-stretchy-h>mjx-ext>mjx-c{width:0}mjx-stretchy-h>mjx-beg>mjx-c{margin-right:-.1em}mjx-stretchy-h>mjx-end>mjx-c{margin-left:-.1em}mjx-stretchy-v{display:inline-block}mjx-stretchy-v>*{display:block}mjx-stretchy-v>mjx-beg{height:0}mjx-stretchy-v>mjx-end>mjx-c{display:block}mjx-stretchy-v>*>mjx-c{transform:scaleY(1);transform-origin:left center;overflow:hidden}mjx-stretchy-v>mjx-ext{display:block;height:100%;box-sizing:border-box;border:0 solid transparent;overflow:visible clip}mjx-stretchy-v>mjx-ext>mjx-c::before{width:initial;box-sizing:border-box}mjx-stretchy-v>mjx-ext>mjx-c{transform:scaleY(500) translateY(.075em);overflow:visible}mjx-mark{display:inline-block;height:0}mjx-c{display:inline-block}mjx-utext{display:inline-block;padding:.75em 0 .2em}mjx-mi{display:inline-block;text-align:left}mjx-msup{display:inline-block;text-align:left}mjx-mn{display:inline-block;text-align:left}mjx-c::before{display:block;width:0}.MJX-TEX{font-family:MJXZERO,MJXTEX}.TEX-B{font-family:MJXZERO,MJXTEX-B}.TEX-I{font-family:MJXZERO,MJXTEX-I}.TEX-MI{font-family:MJXZERO,MJXTEX-MI}.TEX-BI{font-family:MJXZERO,MJXTEX-BI}.TEX-S1{font-family:MJXZERO,MJXTEX-S1}.TEX-S2{font-family:MJXZERO,MJXTEX-S2}.TEX-S3{font-family:MJXZERO,MJXTEX-S3}.TEX-S4{font-family:MJXZERO,MJXTEX-S4}.TEX-A{font-family:MJXZERO,MJXTEX-A}.TEX-C{font-family:MJXZERO,MJXTEX-C}.TEX-CB{font-family:MJXZERO,MJXTEX-CB}.TEX-FR{font-family:MJXZERO,MJXTEX-FR}.TEX-FRB{font-family:MJXZERO,MJXTEX-FRB}.TEX-SS{font-family:MJXZERO,MJXTEX-SS}.TEX-SSB{font-family:MJXZERO,MJXTEX-SSB}.TEX-SSI{font-family:MJXZERO,MJXTEX-SSI}.TEX-SC{font-family:MJXZERO,MJXTEX-SC}.TEX-T{font-family:MJXZERO,MJXTEX-T}.TEX-V{font-family:MJXZERO,MJXTEX-V}.TEX-VB{font-family:MJXZERO,MJXTEX-VB}mjx-stretchy-h mjx-c,mjx-stretchy-v mjx-c{font-family:MJXZERO,MJXTEX-S1,MJXTEX-S4,MJXTEX,MJXTEX-A!important}@font-face{font-family:MJXZERO;src:url("https://publish.obsidian.md/lib/mathjax/output/chtml/fonts/woff-v2/MathJax_Zero.woff") format("woff")}@font-face{font-family:MJXTEX;src:url("https://publish.obsidian.md/lib/mathjax/output/chtml/fonts/woff-v2/MathJax_Main-Regular.woff") format("woff")}@font-face{font-family:MJXTEX-B;src:url("https://publish.obsidian.md/lib/mathjax/output/chtml/fonts/woff-v2/MathJax_Main-Bold.woff") format("woff")}@font-face{font-family:MJXTEX-I;src:url("https://publish.obsidian.md/lib/mathjax/output/chtml/fonts/woff-v2/MathJax_Math-Italic.woff") format("woff")}@font-face{font-family:MJXTEX-MI;src:url("https://publish.obsidian.md/lib/mathjax/output/chtml/fonts/woff-v2/MathJax_Main-Italic.woff") format("woff")}@font-face{font-family:MJXTEX-BI;src:url("https://publish.obsidian.md/lib/mathjax/output/chtml/fonts/woff-v2/MathJax_Math-BoldItalic.woff") format("woff")}@font-face{font-family:MJXTEX-S1;src:url("https://publish.obsidian.md/lib/mathjax/output/chtml/fonts/woff-v2/MathJax_Size1-Regular.woff") format("woff")}@font-face{font-family:MJXTEX-S2;src:url("https://publish.obsidian.md/lib/mathjax/output/chtml/fonts/woff-v2/MathJax_Size2-Regular.woff") format("woff")}@font-face{font-family:MJXTEX-S3;src:url("https://publish.obsidian.md/lib/mathjax/output/chtml/fonts/woff-v2/MathJax_Size3-Regular.woff") format("woff")}@font-face{font-family:MJXTEX-S4;src:url("https://publish.obsidian.md/lib/mathjax/output/chtml/fonts/woff-v2/MathJax_Size4-Regular.woff") format("woff")}@font-face{font-family:MJXTEX-A;src:url("https://publish.obsidian.md/lib/mathjax/output/chtml/fonts/woff-v2/MathJax_AMS-Regular.woff") format("woff")}@font-face{font-family:MJXTEX-C;src:url("https://publish.obsidian.md/lib/mathjax/output/chtml/fonts/woff-v2/MathJax_Calligraphic-Regular.woff") format("woff")}@font-face{font-family:MJXTEX-CB;src:url("https://publish.obsidian.md/lib/mathjax/output/chtml/fonts/woff-v2/MathJax_Calligraphic-Bold.woff") format("woff")}@font-face{font-family:MJXTEX-FR;src:url("https://publish.obsidian.md/lib/mathjax/output/chtml/fonts/woff-v2/MathJax_Fraktur-Regular.woff") format("woff")}@font-face{font-family:MJXTEX-FRB;src:url("https://publish.obsidian.md/lib/mathjax/output/chtml/fonts/woff-v2/MathJax_Fraktur-Bold.woff") format("woff")}@font-face{font-family:MJXTEX-SS;src:url("https://publish.obsidian.md/lib/mathjax/output/chtml/fonts/woff-v2/MathJax_SansSerif-Regular.woff") format("woff")}@font-face{font-family:MJXTEX-SSB;src:url("https://publish.obsidian.md/lib/mathjax/output/chtml/fonts/woff-v2/MathJax_SansSerif-Bold.woff") format("woff")}@font-face{font-family:MJXTEX-SSI;src:url("https://publish.obsidian.md/lib/mathjax/output/chtml/fonts/woff-v2/MathJax_SansSerif-Italic.woff") format("woff")}@font-face{font-family:MJXTEX-SC;src:url("https://publish.obsidian.md/lib/mathjax/output/chtml/fonts/woff-v2/MathJax_Script-Regular.woff") format("woff")}@font-face{font-family:MJXTEX-T;src:url("https://publish.obsidian.md/lib/mathjax/output/chtml/fonts/woff-v2/MathJax_Typewriter-Regular.woff") format("woff")}@font-face{font-family:MJXTEX-V;src:url("https://publish.obsidian.md/lib/mathjax/output/chtml/fonts/woff-v2/MathJax_Vector-Regular.woff") format("woff")}@font-face{font-family:MJXTEX-VB;src:url("https://publish.obsidian.md/lib/mathjax/output/chtml/fonts/woff-v2/MathJax_Vector-Bold.woff") format("woff")}mjx-c.mjx-c28::before{padding:.75em .389em .25em 0;content:"("}mjx-c.mjx-c1D43C.TEX-I::before{padding:.683em .504em 0 0;content:"I"}mjx-c.mjx-c1D434.TEX-I::before{padding:.716em .75em 0 0;content:"A"}mjx-c.mjx-c29::before{padding:.75em .389em .25em 0;content:")"}mjx-c.mjx-c33::before{padding:.665em .5em .022em 0;content:"3"}</style><div class="markdown-preview-sizer markdown-preview-section" style="min-height:1306px"><div class="markdown-preview-pusher" style="width:1px;height:.1px;margin-bottom:0"></div><div class="mod-header"></div><div><pre class="frontmatter language-yaml" tabindex="0" style="display:none"><code class="language-yaml is-loaded"><span class="token key atrule">tags</span><span class="token punctuation">:</span> project/grey<span class="token punctuation">-</span>llm type/blog type/literature type/note 
<span class="token key atrule">created</span><span class="token punctuation">:</span> <span class="token datetime number">2023-10-28</span>
<span class="token key atrule">lead</span><span class="token punctuation">:</span> Incorporating Large Language Models (LLMs) into production workflows requires careful consideration and adherence to best practices. Data quality and model selection<span class="token punctuation">,</span> evaluation<span class="token punctuation">,</span> memory management<span class="token punctuation">,</span> and privacy concerns all play a vital role in harnessing the full potential of LLMs while delivering reliable and user<span class="token punctuation">-</span>centric applications. Starting with clean and relevant data sets the foundation for success. Leveraging smaller models<span class="token punctuation">,</span> fine<span class="token punctuation">-</span>tuning efficiently<span class="token punctuation">,</span> and embracing traditional ML techniques when appropriate can optimize cost and performance. Use agents and chains judiciously<span class="token punctuation">,</span> focusing on minimizing latency for a seamless user experience. Finally<span class="token punctuation">,</span> prioritize privacy by employing techniques like data anonymization and transparent data usage policies.
<span class="token key atrule">link</span><span class="token punctuation">:</span> https<span class="token punctuation">:</span>//medium.com/@_aigeek/best<span class="token punctuation">-</span>practices<span class="token punctuation">-</span>for<span class="token punctuation">-</span>deploying<span class="token punctuation">-</span>large<span class="token punctuation">-</span>language<span class="token punctuation">-</span>models<span class="token punctuation">-</span>llms<span class="token punctuation">-</span>in<span class="token punctuation">-</span>production<span class="token punctuation">-</span>fdc5bf240d6a</code><button class="copy-code-button">Copy</button></pre></div><div class="heading-wrapper"><h1 data-heading="Best Practices for Deploying Large Language Models (LLMs) in Production" class="heading" id="Best_Practices_for_Deploying_Large_Language_Models_(LLMs)_in_Production"><div class="heading-before"></div>Best Practices for Deploying Large Language Models (LLMs) in Production<div class="heading-after">...</div></h1><div class="heading-children"><div><p><img alt="rw-book-cover" src="https://miro.medium.com/v2/resize:fit:640/1*ZwtGweo0PSFPwUVpDH-p7w.jpeg" referrerpolicy="no-referrer"></p></div><div class="heading-wrapper"><h2 data-heading="Highlights" class="heading" id="Highlights"><div class="heading-before"></div><div class="heading-collapse-indicator collapse-indicator collapse-icon"><svg xmlns="http://www.w3.org/2000/svg" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="svg-icon right-triangle" style="width:24px;max-width:100%"><path d="M3 8L12 17L21 8"></path></svg></div>Highlights<div class="heading-after">...</div></h2><div class="heading-children"><div><ul><li data-line="0">Cost and latency considerations are paramount when deploying LLM applications. Longer prompts increase the <a data-tooltip-position="top" aria-label="https://sunyan.substack.com/p/the-economics-of-large-language-models" rel="noopener" class="external-link" href="https://sunyan.substack.com/p/the-economics-of-large-language-models" target="_blank">cost of inference</a>, while the length of the output directly impacts latency. (<a data-tooltip-position="top" aria-label="https://read.readwise.io/read/01hdpks8kfkaj2s8q14wksfdd1" rel="noopener" class="external-link" href="https://read.readwise.io/read/01hdpks8kfkaj2s8q14wksfdd1" target="_blank">View Highlight</a>)</li><li data-line="1">As a result, it is important to approach the evaluation process with caution and consider multiple perspectives. Human evaluation, where human annotators assess the outputs of the LLM, can provide valuable insights into the quality of the model’s responses. (<a data-tooltip-position="top" aria-label="https://read.readwise.io/read/01hdpm6md4y4kr910djz4br8g8" rel="noopener" class="external-link" href="https://read.readwise.io/read/01hdpm6md4y4kr910djz4br8g8" target="_blank">View Highlight</a>)</li><li data-line="2">Smaller LLMs are both efficient and cost-effective (<a data-tooltip-position="top" aria-label="https://read.readwise.io/read/01ha7qmzevy7344wrmh2dgarry" rel="noopener" class="external-link" href="https://read.readwise.io/read/01ha7qmzevy7344wrmh2dgarry" target="_blank">View Highlight</a>)</li><li data-line="3">Cost of fine-tuning LLMs is going down (<a data-tooltip-position="top" aria-label="https://read.readwise.io/read/01ha7qn4d03gdwfyrmedr46bzb" rel="noopener" class="external-link" href="https://read.readwise.io/read/01ha7qn4d03gdwfyrmedr46bzb" target="_blank">View Highlight</a>)</li><li data-line="4">Evaluating the performance of LLMs is an ongoing challenge in the field. Despite the progress made, <a data-tooltip-position="top" aria-label="https://arxiv.org/abs/2303.06223" rel="noopener" class="external-link" href="https://arxiv.org/abs/2303.06223" target="_blank">evaluation metrics for LLMs are still subjective to some extent</a>. (<a data-tooltip-position="top" aria-label="https://read.readwise.io/read/01ha7qnfs5cy50kdj3gxbpd8z4" rel="noopener" class="external-link" href="https://read.readwise.io/read/01ha7qnfs5cy50kdj3gxbpd8z4" target="_blank">View Highlight</a>)</li><li data-line="5">Before resorting to fine-tuning with smaller models, <a data-tooltip-position="top" aria-label="https://www.union.ai/blog-post/fine-tuning-vs-prompt-tuning-large-language-models" rel="noopener" class="external-link" href="https://www.union.ai/blog-post/fine-tuning-vs-prompt-tuning-large-language-models" target="_blank">exhaust the possibilities of prompt engineering</a> and explore different approaches to maximize the performance of the base model. (<a data-tooltip-position="top" aria-label="https://read.readwise.io/read/01hdpmg7drdrdqtqeg3gv162e9" rel="noopener" class="external-link" href="https://read.readwise.io/read/01hdpmg7drdrdqtqeg3gv162e9" target="_blank">View Highlight</a>)</li><li data-line="6">Choosing the right LLM API and hardware setup, leveraging distributed computing, and employing techniques like caching and batching can significantly reduce response times and ensure a smooth and responsive user experience. (<a data-tooltip-position="top" aria-label="https://read.readwise.io/read/01hdpms1xms7v9tatfxsn70yp7" rel="noopener" class="external-link" href="https://read.readwise.io/read/01hdpms1xms7v9tatfxsn70yp7" target="_blank">View Highlight</a>)</li><li data-line="7">Vector databases are becoming standards for developing data aware AI apps (<a data-tooltip-position="top" aria-label="https://read.readwise.io/read/01ha7qnrxx43wh2a5zwd4egx0c" rel="noopener" class="external-link" href="https://read.readwise.io/read/01ha7qnrxx43wh2a5zwd4egx0c" target="_blank">View Highlight</a>)</li><li data-line="8">Prioritize prompt engineering before engaging in use case-specific fine-tuning (<a data-tooltip-position="top" aria-label="https://read.readwise.io/read/01ha7qnwk04qr138ha9smg37b6" rel="noopener" class="external-link" href="https://read.readwise.io/read/01ha7qnwk04qr138ha9smg37b6" target="_blank">View Highlight</a>)</li><li data-line="9">While agents and chains can enhance the capabilities of LLMs, <a data-tooltip-position="top" aria-label="https://matt-rickard.com/autonomous-llm-agents-are-at-least-10-years-out" rel="noopener" class="external-link" href="https://matt-rickard.com/autonomous-llm-agents-are-at-least-10-years-out" target="_blank">they should be used judiciously</a>. Agents like <a data-tooltip-position="top" aria-label="https://github.com/yoheinakajima/babyagi" rel="noopener" class="external-link" href="https://github.com/yoheinakajima/babyagi" target="_blank">BabyAGI</a> and <a data-tooltip-position="top" aria-label="https://github.com/Significant-Gravitas/Auto-GPT" rel="noopener" class="external-link" href="https://github.com/Significant-Gravitas/Auto-GPT" target="_blank">AutoGPT</a> are supposed to goal-driven self-executing software that use the LLM to provide specialized functionality, such as searching the web and executing python scripts. Chains, on the other hand, are sequences of multiple LLMs working in tandem to accomplish complex tasks. A well known chaining framework is <a data-tooltip-position="top" aria-label="https://docs.langchain.com/docs/" rel="noopener" class="external-link" href="https://docs.langchain.com/docs/" target="_blank">LangChain</a>. (<a data-tooltip-position="top" aria-label="https://read.readwise.io/read/01ha7qp39ryyb66b7mqrycnq60" rel="noopener" class="external-link" href="https://read.readwise.io/read/01ha7qp39ryyb66b7mqrycnq60" target="_blank">View Highlight</a>)</li><li data-line="10">Low latency is key for a seamless user experience (<a data-tooltip-position="top" aria-label="https://read.readwise.io/read/01ha7qp8h8e4q6w7gs3hr37pfy" rel="noopener" class="external-link" href="https://read.readwise.io/read/01ha7qp8h8e4q6w7gs3hr37pfy" target="_blank">View Highlight</a>)</li><li data-line="11">Data privacy is on top of everyone’s mind (<a data-tooltip-position="top" aria-label="https://read.readwise.io/read/01ha7qpbf2wxj6sbv0sveah4xr" rel="noopener" class="external-link" href="https://read.readwise.io/read/01ha7qpbf2wxj6sbv0sveah4xr" target="_blank">View Highlight</a>)</li></ul></div></div></div><div class="heading-wrapper"><h2 data-heading="New highlights added 2023-11-16 at 3:35 PM" class="heading" id="New_highlights_added_2023-11-16_at_3:35_PM"><div class="heading-before"></div><div class="heading-collapse-indicator collapse-indicator collapse-icon"><svg xmlns="http://www.w3.org/2000/svg" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="svg-icon right-triangle" style="width:24px;max-width:100%"><path d="M3 8L12 17L21 8"></path></svg></div>New highlights added 2023-11-16 at 3:35 PM<div class="heading-after">...</div></h2><div class="heading-children"><div><ul><li data-line="0">Contrary to popular belief, <a data-tooltip-position="top" aria-label="https://www.nature.com/articles/d41586-023-00641-w" rel="noopener" class="external-link" href="https://www.nature.com/articles/d41586-023-00641-w" target="_blank">bigger doesn’t always mean better when it comes to LLMs</a>. Smaller models can be just as effective, if not more so, when it comes to specific tasks. In fact, using smaller models tailored to a specific task can offer several advantages. First and foremost, <a data-tooltip-position="top" aria-label="https://www.mosaicml.com/blog/gpt-3-quality-for-500k" rel="noopener" class="external-link" href="https://www.mosaicml.com/blog/gpt-3-quality-for-500k" target="_blank">smaller models are often more cost-effective to train and deploy</a>. They require fewer computational resources, making them an attractive option, especially for resource-constrained projects. (<a data-tooltip-position="top" aria-label="https://read.readwise.io/read/01hfaa90p8wb19bb0s5fm9mjs0" rel="noopener" class="external-link" href="https://read.readwise.io/read/01hfaa90p8wb19bb0s5fm9mjs0" target="_blank">View Highlight</a>)</li></ul></div><div class="mod-footer"></div></div></div></div></div></div></div></div><div class="sidebar-right sidebar"><div class="sidebar-gutter"><div class="clickable-icon sidebar-collapse-icon"><svg xmlns="http://www.w3.org/2000/svg" width="100%" height="100%" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="3" stroke-linecap="round" stroke-linejoin="round" class="svg-icon"><path d="M21 3H3C1.89543 3 1 3.89543 1 5V19C1 20.1046 1.89543 21 3 21H21C22.1046 21 23 20.1046 23 19V5C23 3.89543 22.1046 3 21 3Z"></path><path d="M10 4V20"></path><path d="M4 7H7"></path><path d="M4 10H7"></path><path d="M4 13H7"></path></svg></div></div><div class="sidebar-container"><div class="sidebar-sizer"><div class="sidebar-content-positioner"><div class="sidebar-content"><div class="graph-view-wrapper"><div class="sidebar-section-header">Interactive Graph</div><div class="graph-view-placeholder"><div class="graph-view-container"><div class="graph-icon graph-expand" role="button" aria-label="Expand" data-tooltip-position="top"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="svg-icon"><line x1="7" y1="17" x2="17" y2="7"></line><polyline points="7 7 17 7 17 17"></polyline></svg></div><canvas id="graph-canvas" width="512px" height="512px"></canvas></div></div></div><div class="tree-container outline-tree" data-depth="0"><div class="tree-header"><span class="sidebar-section-header">Table Of Contents</span><button class="clickable-icon collapse-tree-button"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"></svg></button></div><div class="tree-scroll-area"><div class="tree-item" data-depth="1"><div class="tree-item-contents"><a class="internal-link tree-item-link" href="#Best_Practices_for_Deploying_Large_Language_Models_(LLMs)_in_Production"><span class="tree-item-title"><p>Best Practices for Deploying Large Language Models (LLMs) in Production</p></span></a></div><div class="tree-item-children"><div class="tree-item" data-depth="2"><div class="tree-item-contents"><a class="internal-link tree-item-link" href="#Highlights"><span class="tree-item-title"><p>Highlights</p></span></a></div><div class="tree-item-children"></div></div><div class="tree-item" data-depth="2"><div class="tree-item-contents"><a class="internal-link tree-item-link" href="#New_highlights_added_2023-11-16_at_3:35_PM"><span class="tree-item-title"><p>New highlights added 2023-11-16 at 3:35 PM</p></span></a></div><div class="tree-item-children"></div></div></div></div></div></div></div></div></div></div></div></div></body></html>