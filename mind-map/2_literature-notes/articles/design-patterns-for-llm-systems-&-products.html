<!doctype html><html><head><title>Design Patterns for LLM Systems &amp; Products</title><base href="../../"><meta id="root-path" root-path="../../"><link rel="icon" sizes="96x96" href="https://publish-01.obsidian.md/access/f786db9fac45774fa4f0d8112e232d67/favicon-96x96.png"><meta name="viewport" content="width=device-width,initial-scale=1,user-scalable=yes,minimum-scale=1,maximum-scale=5"><meta charset="UTF-8"><link rel="stylesheet" href="lib/styles/obsidian-styles.css"><link rel="stylesheet" href="lib/styles/theme.css"><link rel="stylesheet" href="lib/styles/plugin-styles.css"><link rel="stylesheet" href="lib/styles/snippets.css"><link rel="stylesheet" href="lib/styles/generated-styles.css"><style>body.css-settings-manager{--heading-spacing:0}</style><script type="module" src="lib/scripts/graph_view.js"></script><script src="lib/scripts/graph_wasm.js"></script><script src="lib/scripts/tinycolor.js"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/pixi.js/7.2.4/pixi.min.js" integrity="sha512-Ch/O6kL8BqUwAfCF7Ie5SX1Hin+BJgYH4pNjRqXdTEqMsis1TUYg+j6nnI9uduPjGaj7DN4UKCZgpvoExt6dkw==" crossorigin="anonymous" referrerpolicy="no-referrer"></script><script src="lib/scripts/webpage.js"></script><script src="lib/scripts/generated.js"></script></head><body class="theme-dark mod-macos native-scrollbars show-inline-title minimal-dracula-dark colorful-active system-shade minimal-dark-black callouts-default trim-cols checkbox-circle pdf-seamless-on pdf-invert-dark pdf-blend-light metadata-heading-off sidebar-tabs-default ribbon-hidden maximize-tables-off tabs-default tab-stack-top minimal-tab-title-hover is-fullscreen loading"><div class="webpage-container"><div class="sidebar-left sidebar"><div class="sidebar-container"><div class="sidebar-sizer"><div class="sidebar-content-positioner"><div class="sidebar-content"><div><label class="theme-toggle-container" for="theme_toggle"><input class="theme-toggle-input" type="checkbox" id="theme_toggle"><div class="toggle-background"></div></label></div><div class="tree-container file-tree mod-nav-indicator" data-depth="0"><div class="tree-header"><span class="sidebar-section-header">obsidian-notes</span><button class="clickable-icon collapse-tree-button is-collapsed"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"></svg></button></div><div class="tree-scroll-area"><div class="tree-item mod-tree-folder mod-collapsible is-collapsed" data-depth="1"><div class="tree-item-contents"><a class="internal-link tree-item-link"><div class="tree-item-icon collapse-icon"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="svg-icon right-triangle"><path d="M3 8L12 17L21 8"></path></svg></div><span class="tree-item-title">2_Literature Notes</span></a></div><div class="tree-item-children" style="display:none"><div class="tree-item mod-tree-folder mod-collapsible is-collapsed" data-depth="2"><div class="tree-item-contents"><a class="internal-link tree-item-link"><div class="tree-item-icon collapse-icon"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="svg-icon right-triangle"><path d="M3 8L12 17L21 8"></path></svg></div><span class="tree-item-title">Articles</span></a></div><div class="tree-item-children" style="display:none"><div class="tree-item mod-tree-file" data-depth="3"><div class="tree-item-contents"><a class="internal-link tree-item-link" href="2_literature-notes/articles/3-things-chatgpt-needs-before-it-can-be-deployed-in-customer-service.html"><span class="tree-item-title">3 Things ChatGPT Needs Before it Can be Deployed in Customer Service</span></a></div><div class="tree-item-children"></div></div><div class="tree-item mod-tree-file" data-depth="3"><div class="tree-item-contents"><a class="internal-link tree-item-link" href="2_literature-notes/articles/a-developer’s-guide-to-llmops-operationalizing-llms-at-scale.html"><span class="tree-item-title">A Developer’s Guide To LLMOps Operationalizing LLMs At Scale</span></a></div><div class="tree-item-children"></div></div><div class="tree-item mod-tree-file" data-depth="3"><div class="tree-item-contents"><a class="internal-link tree-item-link" href="2_literature-notes/articles/age-of-industrialized-ai-dan-jeffries-llms-in-production-conference.html"><span class="tree-item-title">Age of Industrialized AI Dan Jeffries LLMs in Production Conference</span></a></div><div class="tree-item-children"></div></div><div class="tree-item mod-tree-file" data-depth="3"><div class="tree-item-contents"><a class="internal-link tree-item-link" href="2_literature-notes/articles/ai-agents-when-and-how-to-implement.html"><span class="tree-item-title">AI Agents When and How to Implement</span></a></div><div class="tree-item-children"></div></div><div class="tree-item mod-tree-file" data-depth="3"><div class="tree-item-contents"><a class="internal-link tree-item-link" href="2_literature-notes/articles/best-practices-for-deploying-language-models.html"><span class="tree-item-title">Best Practices for Deploying Language Models</span></a></div><div class="tree-item-children"></div></div><div class="tree-item mod-tree-file" data-depth="3"><div class="tree-item-contents"><a class="internal-link tree-item-link" href="2_literature-notes/articles/best-practices-for-deploying-large-language-models-(llms)-in-production.html"><span class="tree-item-title">Best Practices for Deploying Large Language Models (LLMs) in Production</span></a></div><div class="tree-item-children"></div></div><div class="tree-item mod-tree-file" data-depth="3"><div class="tree-item-contents"><a class="internal-link tree-item-link" href="2_literature-notes/articles/best-practices-for-large-language-model-(llm)-deployment.html"><span class="tree-item-title">Best Practices for Large Language Model (LLM) Deployment</span></a></div><div class="tree-item-children"></div></div><div class="tree-item mod-tree-file" data-depth="3"><div class="tree-item-contents"><a class="internal-link tree-item-link" href="2_literature-notes/articles/best-practices-for-monitoring-large-language-models.html"><span class="tree-item-title">Best Practices for Monitoring Large Language Models</span></a></div><div class="tree-item-children"></div></div><div class="tree-item mod-tree-file" data-depth="3"><div class="tree-item-contents"><a class="internal-link tree-item-link" href="2_literature-notes/articles/building-llm-applications-for-production-chip-huyen-llms-in-prod-conference.html"><span class="tree-item-title">Building LLM Applications for Production Chip Huyen LLMs in Prod Conference</span></a></div><div class="tree-item-children"></div></div><div class="tree-item mod-tree-file" data-depth="3"><div class="tree-item-contents"><a class="internal-link tree-item-link" href="2_literature-notes/articles/comply-or-die-the-rise-of-the-ai-governance-stack.html"><span class="tree-item-title">Comply or Die The Rise of the AI Governance Stack</span></a></div><div class="tree-item-children"></div></div><div class="tree-item mod-tree-file" data-depth="3"><div class="tree-item-contents"><a class="internal-link tree-item-link" href="2_literature-notes/articles/concepts-for-reliability-of-llms-in-production.html"><span class="tree-item-title">Concepts for Reliability of LLMs in Production</span></a></div><div class="tree-item-children"></div></div><div class="tree-item mod-tree-file" data-depth="3"><div class="tree-item-contents"><a class="internal-link tree-item-link" href="2_literature-notes/articles/controlled-and-compliant-ai-applications-daniel-whitenack-llms-in-production-conference-part-2.html"><span class="tree-item-title">Controlled and Compliant AI Applications Daniel Whitenack LLMs in Production Conference Part 2</span></a></div><div class="tree-item-children"></div></div><div class="tree-item mod-tree-file" data-depth="3"><div class="tree-item-contents"><a class="internal-link tree-item-link" href="2_literature-notes/articles/deploying-large-language-models-in-production-llm-deployment-challenges.html"><span class="tree-item-title">Deploying Large Language Models in Production LLM Deployment Challenges</span></a></div><div class="tree-item-children"></div></div><div class="tree-item mod-tree-file" data-depth="3"><div class="tree-item-contents"><a class="internal-link tree-item-link" href="2_literature-notes/articles/deploying-large-language-models-in-production-orchestrating-llms.html"><span class="tree-item-title">Deploying Large Language Models in Production Orchestrating LLMs</span></a></div><div class="tree-item-children"></div></div><div class="tree-item mod-tree-file" data-depth="3"><div class="tree-item-contents"><a class="internal-link tree-item-link" href="2_literature-notes/articles/deploying-large-language-models-in-production-the-anatomy-of-llm-applications.html"><span class="tree-item-title">Deploying Large Language Models in Production The Anatomy of LLM Applications</span></a></div><div class="tree-item-children"></div></div><div class="tree-item mod-tree-file" data-depth="3"><div class="tree-item-contents"><a class="internal-link tree-item-link" href="2_literature-notes/articles/design-patterns-for-llm-systems-&amp;-products.html"><span class="tree-item-title">Design Patterns for LLM Systems &amp; Products</span></a></div><div class="tree-item-children"></div></div><div class="tree-item mod-tree-file" data-depth="3"><div class="tree-item-contents"><a class="internal-link tree-item-link" href="2_literature-notes/articles/devtools-for-language-models-—-predicting-the-future.html"><span class="tree-item-title">DevTools for Language Models — Predicting the Future</span></a></div><div class="tree-item-children"></div></div><div class="tree-item mod-tree-file" data-depth="3"><div class="tree-item-contents"><a class="internal-link tree-item-link" href="2_literature-notes/articles/edition-21-a-framework-to-securely-use-llms-in-companies-part-1-overview-of-risks.html"><span class="tree-item-title">Edition 21 A Framework to Securely Use LLMs in Companies - Part 1 Overview of Risks</span></a></div><div class="tree-item-children"></div></div><div class="tree-item mod-tree-file" data-depth="3"><div class="tree-item-contents"><a class="internal-link tree-item-link" href="2_literature-notes/articles/edition-22-a-framework-to-securely-use-llms-in-companies-part-2-managing-risk.html"><span class="tree-item-title">Edition 22 A Framework to Securely Use LLMs in Companies - Part 2 Managing Risk</span></a></div><div class="tree-item-children"></div></div><div class="tree-item mod-tree-file" data-depth="3"><div class="tree-item-contents"><a class="internal-link tree-item-link" href="2_literature-notes/articles/emerging-architectures-for-llm-applications.html"><span class="tree-item-title">Emerging Architectures for LLM Applications</span></a></div><div class="tree-item-children"></div></div><div class="tree-item mod-tree-file" data-depth="3"><div class="tree-item-contents"><a class="internal-link tree-item-link" href="2_literature-notes/articles/emerging-architectures-for-llms-applications-datasciencedojo.html"><span class="tree-item-title">Emerging Architectures for LLMs Applications - datasciencedojo</span></a></div><div class="tree-item-children"></div></div><div class="tree-item mod-tree-file" data-depth="3"><div class="tree-item-contents"><a class="internal-link tree-item-link" href="2_literature-notes/articles/emerging-patterns-for-llms-in-production-willem-pienaar-llms-in-prod-conference-lightning-talk.html"><span class="tree-item-title">Emerging Patterns for LLMs in Production Willem Pienaar LLMs in Prod Conference Lightning Talk</span></a></div><div class="tree-item-children"></div></div><div class="tree-item mod-tree-file" data-depth="3"><div class="tree-item-contents"><a class="internal-link tree-item-link" href="2_literature-notes/articles/ensuring-accuracy-and-quality-in-llm-driven-products-adam-nolte-llms-in-prod-conference.html"><span class="tree-item-title">Ensuring Accuracy and Quality in LLM-driven Products Adam Nolte LLMs in Prod Conference</span></a></div><div class="tree-item-children"></div></div><div class="tree-item mod-tree-file" data-depth="3"><div class="tree-item-contents"><a class="internal-link tree-item-link" href="2_literature-notes/articles/evaluating-rag-pipelines-with-ragas-+-langsmith.html"><span class="tree-item-title">Evaluating RAG Pipelines With Ragas + LangSmith</span></a></div><div class="tree-item-children"></div></div><div class="tree-item mod-tree-file" data-depth="3"><div class="tree-item-contents"><a class="internal-link tree-item-link" href="2_literature-notes/articles/exploring-llm-apps-the-langchain-paradigm-and-future-alternatives.html"><span class="tree-item-title">Exploring LLM Apps The LangChain Paradigm and Future Alternatives</span></a></div><div class="tree-item-children"></div></div><div class="tree-item mod-tree-file" data-depth="3"><div class="tree-item-contents"><a class="internal-link tree-item-link" href="2_literature-notes/articles/finetuning-large-language-models.html"><span class="tree-item-title">Finetuning Large Language Models</span></a></div><div class="tree-item-children"></div></div><div class="tree-item mod-tree-file" data-depth="3"><div class="tree-item-contents"><a class="internal-link tree-item-link" href="2_literature-notes/articles/fmopsllmops-operationalize-generative-ai-and-differences-with-mlops.html"><span class="tree-item-title">FMOpsLLMOps Operationalize Generative AI and Differences With MLOps</span></a></div><div class="tree-item-children"></div></div><div class="tree-item mod-tree-file" data-depth="3"><div class="tree-item-contents"><a class="internal-link tree-item-link" href="2_literature-notes/articles/four-ways-that-enterprises-deploy-llms.html"><span class="tree-item-title">Four Ways That Enterprises Deploy LLMs</span></a></div><div class="tree-item-children"></div></div><div class="tree-item mod-tree-file" data-depth="3"><div class="tree-item-contents"><a class="internal-link tree-item-link" href="2_literature-notes/articles/from-concept-to-practice-learnings-from-llms-for-enterprise-production-–-part-0.html"><span class="tree-item-title">From Concept to Practice Learnings From LLMs for Enterprise Production – Part 0</span></a></div><div class="tree-item-children"></div></div><div class="tree-item mod-tree-file" data-depth="3"><div class="tree-item-contents"><a class="internal-link tree-item-link" href="2_literature-notes/articles/from-out-of-the-box-to-tailor-made-developing-and-deploying-enterprise-generative-ai-tools.html"><span class="tree-item-title">From Out-of-the-Box to Tailor-Made Developing and Deploying Enterprise Generative AI Tools</span></a></div><div class="tree-item-children"></div></div><div class="tree-item mod-tree-file" data-depth="3"><div class="tree-item-contents"><a class="internal-link tree-item-link" href="2_literature-notes/articles/generative-ai-a-creative-new-world.html"><span class="tree-item-title">Generative AI A Creative New World</span></a></div><div class="tree-item-children"></div></div><div class="tree-item mod-tree-file" data-depth="3"><div class="tree-item-contents"><a class="internal-link tree-item-link" href="2_literature-notes/articles/generative-ai-is-exploding-these-are-the-most-important-trends-to-know.html"><span class="tree-item-title">Generative AI Is Exploding. These Are the Most Important Trends to Know</span></a></div><div class="tree-item-children"></div></div><div class="tree-item mod-tree-file" data-depth="3"><div class="tree-item-contents"><a class="internal-link tree-item-link" href="2_literature-notes/articles/generative-ai’s-act-two.html"><span class="tree-item-title">Generative AI’s Act Two</span></a></div><div class="tree-item-children"></div></div><div class="tree-item mod-tree-file" data-depth="3"><div class="tree-item-contents"><a class="internal-link tree-item-link" href="2_literature-notes/articles/guardrails-for-llms-a-practical-approach-shreya-rajpal-llms-in-prod-conference-part-2.html"><span class="tree-item-title">Guardrails for LLMs A Practical Approach Shreya Rajpal LLMs in Prod Conference Part 2</span></a></div><div class="tree-item-children"></div></div><div class="tree-item mod-tree-file" data-depth="3"><div class="tree-item-contents"><a class="internal-link tree-item-link" href="2_literature-notes/articles/guardrails-what-are-they-and-how-can-you-use-nemo-and-guardrails-ai-to-safeguard-llms.html"><span class="tree-item-title">Guardrails What Are They and How Can You Use NeMo and Guardrails AI to Safeguard LLMs</span></a></div><div class="tree-item-children"></div></div><div class="tree-item mod-tree-file" data-depth="3"><div class="tree-item-contents"><a class="internal-link tree-item-link" href="2_literature-notes/articles/guiding-llms-while-staying-in-the-driver's-seat-jacob-van-gogh-llms-in-prod-con-lightning-talk.html"><span class="tree-item-title">Guiding LLMs While Staying in the Driver's Seat Jacob Van Gogh LLMs in Prod Con Lightning Talk</span></a></div><div class="tree-item-children"></div></div><div class="tree-item mod-tree-file" data-depth="3"><div class="tree-item-contents"><a class="internal-link tree-item-link" href="2_literature-notes/articles/harry-browne’s-17-golden-rules-of-financial-safety.html"><span class="tree-item-title">Harry Browne’s 17 Golden Rules of Financial Safety</span></a></div><div class="tree-item-children"></div></div><div class="tree-item mod-tree-file" data-depth="3"><div class="tree-item-contents"><a class="internal-link tree-item-link" href="2_literature-notes/articles/how-ray-solves-common-production-challenges-for-generative-ai-infrastructure.html"><span class="tree-item-title">How Ray Solves Common Production Challenges for Generative AI Infrastructure</span></a></div><div class="tree-item-children"></div></div><div class="tree-item mod-tree-file" data-depth="3"><div class="tree-item-contents"><a class="internal-link tree-item-link" href="2_literature-notes/articles/how-to-evaluate-your-llm-applications.html"><span class="tree-item-title">How to Evaluate Your LLM Applications</span></a></div><div class="tree-item-children"></div></div><div class="tree-item mod-tree-file" data-depth="3"><div class="tree-item-contents"><a class="internal-link tree-item-link" href="2_literature-notes/articles/illustrating-reinforcement-learning-from-human-feedback.html"><span class="tree-item-title">Illustrating Reinforcement Learning From Human Feedback</span></a></div><div class="tree-item-children"></div></div><div class="tree-item mod-tree-file" data-depth="3"><div class="tree-item-contents"><a class="internal-link tree-item-link" href="2_literature-notes/articles/improving-llms-in-production-with-observability.html"><span class="tree-item-title">Improving LLMs in Production With Observability</span></a></div><div class="tree-item-children"></div></div><div class="tree-item mod-tree-file" data-depth="3"><div class="tree-item-contents"><a class="internal-link tree-item-link" href="2_literature-notes/articles/introduction-to-retrieval-augmented-generation.html"><span class="tree-item-title">Introduction to Retrieval Augmented Generation</span></a></div><div class="tree-item-children"></div></div><div class="tree-item mod-tree-file" data-depth="3"><div class="tree-item-contents"><a class="internal-link tree-item-link" href="2_literature-notes/articles/llm-deployment-with-nlp-models-meryem-arik-llms-in-production-conference-lightning-talk-2.html"><span class="tree-item-title">LLM Deployment With NLP Models Meryem Arik LLMs in Production Conference Lightning Talk 2</span></a></div><div class="tree-item-children"></div></div><div class="tree-item mod-tree-file" data-depth="3"><div class="tree-item-contents"><a class="internal-link tree-item-link" href="2_literature-notes/articles/llm-evaluation-assessing-large-language-models-using-their-peers.html"><span class="tree-item-title">LLM Evaluation Assessing Large Language Models Using Their Peers</span></a></div><div class="tree-item-children"></div></div><div class="tree-item mod-tree-file" data-depth="3"><div class="tree-item-contents"><a class="internal-link tree-item-link" href="2_literature-notes/articles/llm-observability-one-small-step-for-spans,-one-giant-leap-for-span-kinds.html"><span class="tree-item-title">LLM Observability One Small Step for Spans, One Giant Leap for Span-Kinds</span></a></div><div class="tree-item-children"></div></div><div class="tree-item mod-tree-file" data-depth="3"><div class="tree-item-contents"><a class="internal-link tree-item-link" href="2_literature-notes/articles/llmops-the-future-of-mlops-for-generative-ai.html"><span class="tree-item-title">LLMOps The Future of MLOps for Generative AI</span></a></div><div class="tree-item-children"></div></div><div class="tree-item mod-tree-file" data-depth="3"><div class="tree-item-contents"><a class="internal-link tree-item-link" href="2_literature-notes/articles/ml's-hidden-tasks-a-checklist-for-developers-when-building-ml-systems.html"><span class="tree-item-title">ML's Hidden Tasks A Checklist for Developers When Building ML Systems</span></a></div><div class="tree-item-children"></div></div><div class="tree-item mod-tree-file" data-depth="3"><div class="tree-item-contents"><a class="internal-link tree-item-link" href="2_literature-notes/articles/monitoring-llms-metrics,-challenges,-&amp;-hallucinations.html"><span class="tree-item-title">Monitoring LLMs Metrics, Challenges, &amp; Hallucinations</span></a></div><div class="tree-item-children"></div></div><div class="tree-item mod-tree-file" data-depth="3"><div class="tree-item-contents"><a class="internal-link tree-item-link" href="2_literature-notes/articles/no-rose-without-a-thorn-obstacles-to-successful-llm-deployments-tanmay-chopra-llms-in-prod.html"><span class="tree-item-title">No Rose Without a Thorn - Obstacles to Successful LLM Deployments Tanmay Chopra LLMs in Prod</span></a></div><div class="tree-item-children"></div></div><div class="tree-item mod-tree-file" data-depth="3"><div class="tree-item-contents"><a class="internal-link tree-item-link" href="2_literature-notes/articles/nvidia-enables-trustworthy,-safe,-and-secure-large-language-model-conversational-systems.html"><span class="tree-item-title">NVIDIA Enables Trustworthy, Safe, and Secure Large Language Model Conversational Systems</span></a></div><div class="tree-item-children"></div></div><div class="tree-item mod-tree-file" data-depth="3"><div class="tree-item-contents"><a class="internal-link tree-item-link" href="2_literature-notes/articles/production-deployment-checklist-for-machine-learning-models.html"><span class="tree-item-title">Production Deployment Checklist for Machine Learning Models</span></a></div><div class="tree-item-children"></div></div><div class="tree-item mod-tree-file" data-depth="3"><div class="tree-item-contents"><a class="internal-link tree-item-link" href="2_literature-notes/articles/security-challenges-in-llm-adoption-for-enterprises-and-how-to-solve-them.html"><span class="tree-item-title">Security Challenges in LLM Adoption for Enterprises and How to Solve Them</span></a></div><div class="tree-item-children"></div></div><div class="tree-item mod-tree-file" data-depth="3"><div class="tree-item-contents"><a class="internal-link tree-item-link" href="2_literature-notes/articles/the-confidence-checklist-for-llms-in-production-rohit-agarwal-llms-in-prod-conference-part-2.html"><span class="tree-item-title">The Confidence Checklist for LLMs in Production Rohit Agarwal LLMs in Prod Conference Part 2</span></a></div><div class="tree-item-children"></div></div><div class="tree-item mod-tree-file" data-depth="3"><div class="tree-item-contents"><a class="internal-link tree-item-link" href="2_literature-notes/articles/the-generative-ai-life-cycle.html"><span class="tree-item-title">The Generative AI Life-Cycle</span></a></div><div class="tree-item-children"></div></div><div class="tree-item mod-tree-file" data-depth="3"><div class="tree-item-contents"><a class="internal-link tree-item-link" href="2_literature-notes/articles/the-gradient-of-generative-ai-release-methods-and-considerations.html"><span class="tree-item-title">The Gradient of Generative AI Release Methods and Considerations</span></a></div><div class="tree-item-children"></div></div><div class="tree-item mod-tree-file" data-depth="3"><div class="tree-item-contents"><a class="internal-link tree-item-link" href="2_literature-notes/articles/the-new-language-model-stack.html"><span class="tree-item-title">The New Language Model Stack</span></a></div><div class="tree-item-children"></div></div><div class="tree-item mod-tree-file" data-depth="3"><div class="tree-item-contents"><a class="internal-link tree-item-link" href="2_literature-notes/articles/the_practical_guide_deploying_large_language_models.html"><span class="tree-item-title">The_Practical_Guide_Deploying_Large_Language_Models</span></a></div><div class="tree-item-children"></div></div><div class="tree-item mod-tree-file" data-depth="3"><div class="tree-item-contents"><a class="internal-link tree-item-link" href="2_literature-notes/articles/understanding-llmops-large-language-model-operations.html"><span class="tree-item-title">Understanding LLMOps Large Language Model Operations</span></a></div><div class="tree-item-children"></div></div></div></div><div class="tree-item mod-tree-file" data-depth="2"><div class="tree-item-contents"><a class="internal-link tree-item-link" href="2_literature-notes/glr-–-references.html"><span class="tree-item-title">GLR – References</span></a></div><div class="tree-item-children"></div></div></div></div><div class="tree-item mod-tree-file" data-depth="1"><div class="tree-item-contents"><a class="internal-link tree-item-link" href="mind-map-–-generative-ai-release-checklist-3.html"><span class="tree-item-title">Mind Map – Generative AI Release Checklist 3</span></a></div><div class="tree-item-children"></div></div></div></div></div></div></div></div><div class="sidebar-gutter"><div class="clickable-icon sidebar-collapse-icon"><svg xmlns="http://www.w3.org/2000/svg" width="100%" height="100%" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="3" stroke-linecap="round" stroke-linejoin="round" class="svg-icon"><path d="M21 3H3C1.89543 3 1 3.89543 1 5V19C1 20.1046 1.89543 21 3 21H21C22.1046 21 23 20.1046 23 19V5C23 3.89543 22.1046 3 21 3Z"></path><path d="M10 4V20"></path><path d="M4 7H7"></path><path d="M4 10H7"></path><path d="M4 13H7"></path></svg></div></div></div><div class="document-container show"><div class="markdown-preview-view markdown-rendered node-insert-event is-readable-line-width allow-fold-headings show-indentation-guide allow-fold-lists show-properties" style="tab-size:4"><style id="MJX-CHTML-styles">mjx-container[jax=CHTML]{line-height:0}mjx-container [space="1"]{margin-left:.111em}mjx-container [space="2"]{margin-left:.167em}mjx-container [space="3"]{margin-left:.222em}mjx-container [space="4"]{margin-left:.278em}mjx-container [space="5"]{margin-left:.333em}mjx-container [rspace="1"]{margin-right:.111em}mjx-container [rspace="2"]{margin-right:.167em}mjx-container [rspace="3"]{margin-right:.222em}mjx-container [rspace="4"]{margin-right:.278em}mjx-container [rspace="5"]{margin-right:.333em}mjx-container [size="s"]{font-size:70.7%}mjx-container [size=ss]{font-size:50%}mjx-container [size=Tn]{font-size:60%}mjx-container [size=sm]{font-size:85%}mjx-container [size=lg]{font-size:120%}mjx-container [size=Lg]{font-size:144%}mjx-container [size=LG]{font-size:173%}mjx-container [size=hg]{font-size:207%}mjx-container [size=HG]{font-size:249%}mjx-container [width=full]{width:100%}mjx-box{display:inline-block}mjx-block{display:block}mjx-itable{display:inline-table}mjx-row{display:table-row}mjx-row>*{display:table-cell}mjx-mtext{display:inline-block}mjx-mstyle{display:inline-block}mjx-merror{display:inline-block;color:red;background-color:#ff0}mjx-mphantom{visibility:hidden}mjx-assistive-mml{top:0;left:0;clip:rect(1px,1px,1px,1px);user-select:none;position:absolute!important;padding:1px 0 0!important;border:0!important;display:block!important;width:auto!important;overflow:hidden!important}mjx-assistive-mml[display=block]{width:100%!important}mjx-math{display:inline-block;text-align:left;line-height:0;text-indent:0;font-style:normal;font-weight:400;font-size:100%;letter-spacing:normal;border-collapse:collapse;overflow-wrap:normal;word-spacing:normal;white-space:nowrap;direction:ltr;padding:1px 0}mjx-container[jax=CHTML][display=true]{display:block;text-align:center;margin:1em 0}mjx-container[jax=CHTML][display=true][width=full]{display:flex}mjx-container[jax=CHTML][display=true] mjx-math{padding:0}mjx-container[jax=CHTML][justify=left]{text-align:left}mjx-container[jax=CHTML][justify=right]{text-align:right}mjx-mo{display:inline-block;text-align:left}mjx-stretchy-h{display:inline-table;width:100%}mjx-stretchy-h>*{display:table-cell;width:0}mjx-stretchy-h>*>mjx-c{display:inline-block;transform:scaleX(1)}mjx-stretchy-h>*>mjx-c::before{display:inline-block;width:initial}mjx-stretchy-h>mjx-ext{overflow:clip visible;width:100%}mjx-stretchy-h>mjx-ext>mjx-c::before{transform:scaleX(500)}mjx-stretchy-h>mjx-ext>mjx-c{width:0}mjx-stretchy-h>mjx-beg>mjx-c{margin-right:-.1em}mjx-stretchy-h>mjx-end>mjx-c{margin-left:-.1em}mjx-stretchy-v{display:inline-block}mjx-stretchy-v>*{display:block}mjx-stretchy-v>mjx-beg{height:0}mjx-stretchy-v>mjx-end>mjx-c{display:block}mjx-stretchy-v>*>mjx-c{transform:scaleY(1);transform-origin:left center;overflow:hidden}mjx-stretchy-v>mjx-ext{display:block;height:100%;box-sizing:border-box;border:0 solid transparent;overflow:visible clip}mjx-stretchy-v>mjx-ext>mjx-c::before{width:initial;box-sizing:border-box}mjx-stretchy-v>mjx-ext>mjx-c{transform:scaleY(500) translateY(.075em);overflow:visible}mjx-mark{display:inline-block;height:0}mjx-c{display:inline-block}mjx-utext{display:inline-block;padding:.75em 0 .2em}mjx-mi{display:inline-block;text-align:left}mjx-msup{display:inline-block;text-align:left}mjx-mn{display:inline-block;text-align:left}mjx-c::before{display:block;width:0}.MJX-TEX{font-family:MJXZERO,MJXTEX}.TEX-B{font-family:MJXZERO,MJXTEX-B}.TEX-I{font-family:MJXZERO,MJXTEX-I}.TEX-MI{font-family:MJXZERO,MJXTEX-MI}.TEX-BI{font-family:MJXZERO,MJXTEX-BI}.TEX-S1{font-family:MJXZERO,MJXTEX-S1}.TEX-S2{font-family:MJXZERO,MJXTEX-S2}.TEX-S3{font-family:MJXZERO,MJXTEX-S3}.TEX-S4{font-family:MJXZERO,MJXTEX-S4}.TEX-A{font-family:MJXZERO,MJXTEX-A}.TEX-C{font-family:MJXZERO,MJXTEX-C}.TEX-CB{font-family:MJXZERO,MJXTEX-CB}.TEX-FR{font-family:MJXZERO,MJXTEX-FR}.TEX-FRB{font-family:MJXZERO,MJXTEX-FRB}.TEX-SS{font-family:MJXZERO,MJXTEX-SS}.TEX-SSB{font-family:MJXZERO,MJXTEX-SSB}.TEX-SSI{font-family:MJXZERO,MJXTEX-SSI}.TEX-SC{font-family:MJXZERO,MJXTEX-SC}.TEX-T{font-family:MJXZERO,MJXTEX-T}.TEX-V{font-family:MJXZERO,MJXTEX-V}.TEX-VB{font-family:MJXZERO,MJXTEX-VB}mjx-stretchy-h mjx-c,mjx-stretchy-v mjx-c{font-family:MJXZERO,MJXTEX-S1,MJXTEX-S4,MJXTEX,MJXTEX-A!important}@font-face{font-family:MJXZERO;src:url("https://publish.obsidian.md/lib/mathjax/output/chtml/fonts/woff-v2/MathJax_Zero.woff") format("woff")}@font-face{font-family:MJXTEX;src:url("https://publish.obsidian.md/lib/mathjax/output/chtml/fonts/woff-v2/MathJax_Main-Regular.woff") format("woff")}@font-face{font-family:MJXTEX-B;src:url("https://publish.obsidian.md/lib/mathjax/output/chtml/fonts/woff-v2/MathJax_Main-Bold.woff") format("woff")}@font-face{font-family:MJXTEX-I;src:url("https://publish.obsidian.md/lib/mathjax/output/chtml/fonts/woff-v2/MathJax_Math-Italic.woff") format("woff")}@font-face{font-family:MJXTEX-MI;src:url("https://publish.obsidian.md/lib/mathjax/output/chtml/fonts/woff-v2/MathJax_Main-Italic.woff") format("woff")}@font-face{font-family:MJXTEX-BI;src:url("https://publish.obsidian.md/lib/mathjax/output/chtml/fonts/woff-v2/MathJax_Math-BoldItalic.woff") format("woff")}@font-face{font-family:MJXTEX-S1;src:url("https://publish.obsidian.md/lib/mathjax/output/chtml/fonts/woff-v2/MathJax_Size1-Regular.woff") format("woff")}@font-face{font-family:MJXTEX-S2;src:url("https://publish.obsidian.md/lib/mathjax/output/chtml/fonts/woff-v2/MathJax_Size2-Regular.woff") format("woff")}@font-face{font-family:MJXTEX-S3;src:url("https://publish.obsidian.md/lib/mathjax/output/chtml/fonts/woff-v2/MathJax_Size3-Regular.woff") format("woff")}@font-face{font-family:MJXTEX-S4;src:url("https://publish.obsidian.md/lib/mathjax/output/chtml/fonts/woff-v2/MathJax_Size4-Regular.woff") format("woff")}@font-face{font-family:MJXTEX-A;src:url("https://publish.obsidian.md/lib/mathjax/output/chtml/fonts/woff-v2/MathJax_AMS-Regular.woff") format("woff")}@font-face{font-family:MJXTEX-C;src:url("https://publish.obsidian.md/lib/mathjax/output/chtml/fonts/woff-v2/MathJax_Calligraphic-Regular.woff") format("woff")}@font-face{font-family:MJXTEX-CB;src:url("https://publish.obsidian.md/lib/mathjax/output/chtml/fonts/woff-v2/MathJax_Calligraphic-Bold.woff") format("woff")}@font-face{font-family:MJXTEX-FR;src:url("https://publish.obsidian.md/lib/mathjax/output/chtml/fonts/woff-v2/MathJax_Fraktur-Regular.woff") format("woff")}@font-face{font-family:MJXTEX-FRB;src:url("https://publish.obsidian.md/lib/mathjax/output/chtml/fonts/woff-v2/MathJax_Fraktur-Bold.woff") format("woff")}@font-face{font-family:MJXTEX-SS;src:url("https://publish.obsidian.md/lib/mathjax/output/chtml/fonts/woff-v2/MathJax_SansSerif-Regular.woff") format("woff")}@font-face{font-family:MJXTEX-SSB;src:url("https://publish.obsidian.md/lib/mathjax/output/chtml/fonts/woff-v2/MathJax_SansSerif-Bold.woff") format("woff")}@font-face{font-family:MJXTEX-SSI;src:url("https://publish.obsidian.md/lib/mathjax/output/chtml/fonts/woff-v2/MathJax_SansSerif-Italic.woff") format("woff")}@font-face{font-family:MJXTEX-SC;src:url("https://publish.obsidian.md/lib/mathjax/output/chtml/fonts/woff-v2/MathJax_Script-Regular.woff") format("woff")}@font-face{font-family:MJXTEX-T;src:url("https://publish.obsidian.md/lib/mathjax/output/chtml/fonts/woff-v2/MathJax_Typewriter-Regular.woff") format("woff")}@font-face{font-family:MJXTEX-V;src:url("https://publish.obsidian.md/lib/mathjax/output/chtml/fonts/woff-v2/MathJax_Vector-Regular.woff") format("woff")}@font-face{font-family:MJXTEX-VB;src:url("https://publish.obsidian.md/lib/mathjax/output/chtml/fonts/woff-v2/MathJax_Vector-Bold.woff") format("woff")}mjx-c.mjx-c28::before{padding:.75em .389em .25em 0;content:"("}mjx-c.mjx-c1D43C.TEX-I::before{padding:.683em .504em 0 0;content:"I"}mjx-c.mjx-c1D434.TEX-I::before{padding:.716em .75em 0 0;content:"A"}mjx-c.mjx-c29::before{padding:.75em .389em .25em 0;content:")"}mjx-c.mjx-c33::before{padding:.665em .5em .022em 0;content:"3"}</style><div class="markdown-preview-sizer markdown-preview-section" style="min-height:1306px"><div class="markdown-preview-pusher" style="width:1px;height:.1px;margin-bottom:0"></div><div class="mod-header"></div><div><pre class="frontmatter language-yaml" tabindex="0" style="display:none"><code class="language-yaml is-loaded"><span class="token key atrule">tags</span><span class="token punctuation">:</span> project/grey<span class="token punctuation">-</span>llm type/blog type/literature type/note 
<span class="token key atrule">created</span><span class="token punctuation">:</span> <span class="token datetime number">2023-10-28</span>
<span class="token key atrule">link</span><span class="token punctuation">:</span> https<span class="token punctuation">:</span>//eugeneyan.com/writing/llm<span class="token punctuation">-</span>patterns/</code><button class="copy-code-button">Copy</button></pre></div><div class="heading-wrapper"><h1 data-heading="Design Patterns for LLM Systems &amp; Products" class="heading" id="Design_Patterns_for_LLM_Systems_&amp;_Products"><div class="heading-before"></div>Design Patterns for LLM Systems &amp; Products<div class="heading-after">...</div></h1><div class="heading-children"><div><p><img alt="rw-book-cover" src="https://eugeneyan.com/assets/og_image/llm-patterns-plain.png" referrerpolicy="no-referrer"></p></div><div class="heading-wrapper"><h2 data-heading="Highlights" class="heading" id="Highlights"><div class="heading-before"></div><div class="heading-collapse-indicator collapse-indicator collapse-icon"><svg xmlns="http://www.w3.org/2000/svg" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="svg-icon right-triangle" style="width:24px;max-width:100%"><path d="M3 8L12 17L21 8"></path></svg></div>Highlights<div class="heading-after">...</div></h2><div class="heading-children"><div><ul><li data-line="0">Evals enable us to measure how well our system or product is doing and detect any regressions. (<a data-tooltip-position="top" aria-label="https://read.readwise.io/read/01hdp9angrnd7mcqy4hdb3qydq" rel="noopener" class="external-link" href="https://read.readwise.io/read/01hdp9angrnd7mcqy4hdb3qydq" target="_blank">View Highlight</a>)</li><li data-line="1">Without evals, we would be flying blind, or would have to visually inspect LLM outputs with each change. (<a data-tooltip-position="top" aria-label="https://read.readwise.io/read/01hdp9b0wjdxvaq96fnhekbkqh" rel="noopener" class="external-link" href="https://read.readwise.io/read/01hdp9b0wjdxvaq96fnhekbkqh" target="_blank">View Highlight</a>)</li><li data-line="2"><strong>There are many benchmarks in the field of language modeling</strong>. Some notable ones are: (<a data-tooltip-position="top" aria-label="https://read.readwise.io/read/01hdp9n6zhcxsh2rf6sqhh6wyc" rel="noopener" class="external-link" href="https://read.readwise.io/read/01hdp9n6zhcxsh2rf6sqhh6wyc" target="_blank">View Highlight</a>)</li><li data-line="3"><strong><a data-tooltip-position="top" aria-label="https://arxiv.org/abs/2009.03300" rel="noopener" class="external-link" href="https://arxiv.org/abs/2009.03300" target="_blank">MMLU</a></strong>: A set of 57 tasks that span elementary math, US history, computer science, law, and more. To perform well, models must possess extensive world knowledge and problem-solving ability. (<a data-tooltip-position="top" aria-label="https://read.readwise.io/read/01hdp9bhppaj2vth60pmvnrm49" rel="noopener" class="external-link" href="https://read.readwise.io/read/01hdp9bhppaj2vth60pmvnrm49" target="_blank">View Highlight</a>)</li><li data-line="4"><strong><a data-tooltip-position="top" aria-label="https://github.com/EleutherAI/lm-evaluation-harness" rel="noopener" class="external-link" href="https://github.com/EleutherAI/lm-evaluation-harness" target="_blank">EleutherAI Eval</a></strong>: Unified framework to test models via zero/few-shot settings on 200 tasks. Incorporates a large number of evals including BigBench, MMLU, etc. (<a data-tooltip-position="top" aria-label="https://read.readwise.io/read/01hdp9gkjzdqn8fq2fa173c6k4" rel="noopener" class="external-link" href="https://read.readwise.io/read/01hdp9gkjzdqn8fq2fa173c6k4" target="_blank">View Highlight</a>)</li><li data-line="5"><strong><a data-tooltip-position="top" aria-label="https://arxiv.org/abs/2211.09110" rel="noopener" class="external-link" href="https://arxiv.org/abs/2211.09110" target="_blank">HELM</a></strong>: Instead of specific tasks and metrics, HELM offers a comprehensive assessment of LLMs by evaluating them across domains. (<a data-tooltip-position="top" aria-label="https://read.readwise.io/read/01hdp9h384r5qd1erf5f906kmm" rel="noopener" class="external-link" href="https://read.readwise.io/read/01hdp9h384r5qd1erf5f906kmm" target="_blank">View Highlight</a>)</li><li data-line="6"><strong><a data-tooltip-position="top" aria-label="https://github.com/tatsu-lab/alpaca_eval" rel="noopener" class="external-link" href="https://github.com/tatsu-lab/alpaca_eval" target="_blank">AlpacaEval</a></strong>: Automated evaluation framework which measures how often a strong LLM (e.g., GPT-4) prefers the output of one model over a reference model. (<a data-tooltip-position="top" aria-label="https://read.readwise.io/read/01hdp9nbzea4yv42tnpc0yez0f" rel="noopener" class="external-link" href="https://read.readwise.io/read/01hdp9nbzea4yv42tnpc0yez0f" target="_blank">View Highlight</a>)</li><li data-line="7"><strong>Context-dependent</strong>: These take context into account. They’re often proposed for a specific task; repurposing them for other tasks will require some adjustment. (<a data-tooltip-position="top" aria-label="https://read.readwise.io/read/01hdp9v586gzgemk9atvp7ehj3" rel="noopener" class="external-link" href="https://read.readwise.io/read/01hdp9v586gzgemk9atvp7ehj3" target="_blank">View Highlight</a>)</li><li data-line="8"><strong>Context-free</strong>: These aren’t tied to the context when evaluating generated output; they only compare the output with the provided gold references. As they’re task agnostic, they’re easier to apply to a wide variety of tasks. (<a data-tooltip-position="top" aria-label="https://read.readwise.io/read/01hdp9v7k4xsaatjy1ds46r8tc" rel="noopener" class="external-link" href="https://read.readwise.io/read/01hdp9v7k4xsaatjy1ds46r8tc" target="_blank">View Highlight</a>)</li><li data-line="9"><strong><a data-tooltip-position="top" aria-label="https://dl.acm.org/doi/10.3115/1073083.1073135" rel="noopener" class="external-link" href="https://dl.acm.org/doi/10.3115/1073083.1073135" target="_blank">BLEU</a> (Bilingual Evaluation Understudy)</strong> is a precision-based metric: It counts the number of n-grams in the generated output that also show up in the reference, and then divides it by the total number of words in the output. (<a data-tooltip-position="top" aria-label="https://read.readwise.io/read/01hdp9vwsvz2hqe9k9x9w4ne5x" rel="noopener" class="external-link" href="https://read.readwise.io/read/01hdp9vwsvz2hqe9k9x9w4ne5x" target="_blank">View Highlight</a>)</li><li data-line="10"><strong><a data-tooltip-position="top" aria-label="https://aclanthology.org/W04-1013/" rel="noopener" class="external-link" href="https://aclanthology.org/W04-1013/" target="_blank">ROUGE</a> (Recall-Oriented Understudy for Gisting Evaluation)</strong>: In contrast to BLEU, ROUGE is recall-oriented. It counts the number of words in the reference that also occur in the output. It’s typically used to assess automatic summarization tasks. (<a data-tooltip-position="top" aria-label="https://read.readwise.io/read/01hdp9x4bst39fxh67gycvk7a9" rel="noopener" class="external-link" href="https://read.readwise.io/read/01hdp9x4bst39fxh67gycvk7a9" target="_blank">View Highlight</a>)</li><li data-line="11">ROUGE-L: This measures the longest common subsequence (LCS) between the output and the reference. It considers sentence-level structure similarity and zeros in on the longest series of co-occurring in-sequence n-grams. (<a data-tooltip-position="top" aria-label="https://read.readwise.io/read/01hdp9y5h4g46x8688h1gtr6n1" rel="noopener" class="external-link" href="https://read.readwise.io/read/01hdp9y5h4g46x8688h1gtr6n1" target="_blank">View Highlight</a>)</li><li data-line="12">ROUGE-S: This measures the skip-bigram between the output and reference. Skip-bigrams are pairs of words that maintain their sentence order regardless of the words that might be sandwiched between them. (<a data-tooltip-position="top" aria-label="https://read.readwise.io/read/01hdpa0256h85dekx75j2tmwrh" rel="noopener" class="external-link" href="https://read.readwise.io/read/01hdpa0256h85dekx75j2tmwrh" target="_blank">View Highlight</a>)</li><li data-line="13"><strong><a data-tooltip-position="top" aria-label="https://arxiv.org/abs/1904.09675" rel="noopener" class="external-link" href="https://arxiv.org/abs/1904.09675" target="_blank">BERTScore</a></strong> is an embedding-based metric that uses cosine similarity to compare each token or n-gram in the generated output with the reference sentence. (<a data-tooltip-position="top" aria-label="https://read.readwise.io/read/01hdpa0a2g6ah1e5z0qq21wksc" rel="noopener" class="external-link" href="https://read.readwise.io/read/01hdpa0a2g6ah1e5z0qq21wksc" target="_blank">View Highlight</a>)</li><li data-line="14">BERTScore is useful because it can account for synonyms and paraphrasing. Simpler metrics like BLEU and ROUGE can’t do this due to their reliance on exact matches. BERTScore has been shown to have better correlation for tasks such as image captioning and machine translation. (<a data-tooltip-position="top" aria-label="https://read.readwise.io/read/01hdpa147ekezf9p9psc2tacyw" rel="noopener" class="external-link" href="https://read.readwise.io/read/01hdpa147ekezf9p9psc2tacyw" target="_blank">View Highlight</a>)</li><li data-line="15"><strong><a data-tooltip-position="top" aria-label="https://arxiv.org/abs/1909.02622" rel="noopener" class="external-link" href="https://arxiv.org/abs/1909.02622" target="_blank">MoverScore</a></strong> also uses contextualized embeddings to compute the distance between tokens in the generated output and reference. But unlike BERTScore, which is based on one-to-one matching (or “high alignment”) of tokens, MoverScore allows for many-to-one matching (or “soft alignment”). (<a data-tooltip-position="top" aria-label="https://read.readwise.io/read/01hdpa2qvknr4xqnrxsq0redhw" rel="noopener" class="external-link" href="https://read.readwise.io/read/01hdpa2qvknr4xqnrxsq0redhw" target="_blank">View Highlight</a>)</li><li data-line="16">BERTScore (left) vs. MoverScore (right; <a data-tooltip-position="top" aria-label="https://arxiv.org/abs/1909.02622" rel="noopener" class="external-link" href="https://arxiv.org/abs/1909.02622" target="_blank">source</a>) (<a data-tooltip-position="top" aria-label="https://read.readwise.io/read/01hdpa30vk7apbvfdhrjas4nks" rel="noopener" class="external-link" href="https://read.readwise.io/read/01hdpa30vk7apbvfdhrjas4nks" target="_blank">View Highlight</a>)</li><li data-line="17">MoverScore enables the mapping of semantically related words in one sequence to their counterparts in another sequence. It does this by solving a constrained optimization problem that finds the minimum effort to transform one text into another. The idea is to measure the distance that words would have to move to convert one sequence to another. (<a data-tooltip-position="top" aria-label="https://read.readwise.io/read/01hdpa4y6bc01abpyfadx1rjca" rel="noopener" class="external-link" href="https://read.readwise.io/read/01hdpa4y6bc01abpyfadx1rjca" target="_blank">View Highlight</a>)</li><li data-line="18">However, there are several pitfalls to using these conventional benchmarks and metrics. (<a data-tooltip-position="top" aria-label="https://read.readwise.io/read/01hdpa59tzf8704bbevwxb3ewt" rel="noopener" class="external-link" href="https://read.readwise.io/read/01hdpa59tzf8704bbevwxb3ewt" target="_blank">View Highlight</a>)</li><li data-line="19">First, there’s <strong>poor correlation between these metrics and human judgments.</strong> BLEU, ROUGE, METEOR, and others have sometimes shown <a data-tooltip-position="top" aria-label="https://arxiv.org/abs/2008.12009" rel="noopener" class="external-link" href="https://arxiv.org/abs/2008.12009" target="_blank">negative correlation with how humans evaluate fluency</a>. In particular, BLEU and ROUGE have <a data-tooltip-position="top" aria-label="https://www.microsoft.com/en-us/research/publication/gpteval-nlg-evaluation-using-gpt-4-with-better-human-alignment/" rel="noopener" class="external-link" href="https://www.microsoft.com/en-us/research/publication/gpteval-nlg-evaluation-using-gpt-4-with-better-human-alignment/" target="_blank">low correlation with tasks that require creativity and diversity</a>. (<a data-tooltip-position="top" aria-label="https://read.readwise.io/read/01hdpa6bxh0vdher96pfaajh3j" rel="noopener" class="external-link" href="https://read.readwise.io/read/01hdpa6bxh0vdher96pfaajh3j" target="_blank">View Highlight</a>)</li><li data-line="20">Second, these metrics often have <strong>poor adaptability to a wider variety of tasks</strong>. Adopting a metric proposed for one task to another is not always prudent. (<a data-tooltip-position="top" aria-label="https://read.readwise.io/read/01hdpa79n6wa6ft32q4mq8ctcb" rel="noopener" class="external-link" href="https://read.readwise.io/read/01hdpa79n6wa6ft32q4mq8ctcb" target="_blank">View Highlight</a>)</li><li data-line="21">Third, even with recent benchmarks such as MMLU, <strong>the same model can get significantly different scores based on the eval implementation</strong>. <a data-tooltip-position="top" aria-label="https://huggingface.co/blog/evaluating-mmlu-leaderboard" rel="noopener" class="external-link" href="https://huggingface.co/blog/evaluating-mmlu-leaderboard" target="_blank">Huggingface compared the original MMLU implementation</a> with the HELM and EleutherAI implementations and found that the same example could have different prompts across various implementations. (<a data-tooltip-position="top" aria-label="https://read.readwise.io/read/01hdpa8phkn8a4cdanyvstzpkn" rel="noopener" class="external-link" href="https://read.readwise.io/read/01hdpa8phkn8a4cdanyvstzpkn" target="_blank">View Highlight</a>)</li><li data-line="22">This means that model metrics aren’t truly comparable—even for the same eval—unless the eval’s implementation is identical down to minute details like prompts and tokenization. (<a data-tooltip-position="top" aria-label="https://read.readwise.io/read/01hdpahag7gekrhrbkw6dmtsem" rel="noopener" class="external-link" href="https://read.readwise.io/read/01hdpahag7gekrhrbkw6dmtsem" target="_blank">View Highlight</a>)</li><li data-line="23">Beyond the traditional evals above, <strong>an emerging trend is to use a strong LLM as a reference-free metric</strong> for generations from other LLMs. This means we may not need human judgments or gold references for evaluation. (<a data-tooltip-position="top" aria-label="https://read.readwise.io/read/01hdpahw9h12za5n5enw41h4p3" rel="noopener" class="external-link" href="https://read.readwise.io/read/01hdpahw9h12za5n5enw41h4p3" target="_blank">View Highlight</a>)</li><li data-line="24"><strong><a data-tooltip-position="top" aria-label="https://arxiv.org/abs/2303.16634" rel="noopener" class="external-link" href="https://arxiv.org/abs/2303.16634" target="_blank">G-Eval</a> is a framework that applies LLMs</strong> with Chain-of-Though (CoT) and a form-filling paradigm to <strong>evaluate LLM outputs</strong>. First, they provide a task introduction and evaluation criteria to an LLM and ask it to generate a CoT of evaluation steps. (<a data-tooltip-position="top" aria-label="https://read.readwise.io/read/01hdpajzajh3hbajgh9h04hp30" rel="noopener" class="external-link" href="https://read.readwise.io/read/01hdpajzajh3hbajgh9h04hp30" target="_blank">View Highlight</a>)</li><li data-line="25">They found that GPT-4 as an evaluator had a high Spearman correlation with human judgments (0.514), outperforming all previous methods. (<a data-tooltip-position="top" aria-label="https://read.readwise.io/read/01hdpaprp603dpr2ngjev5a4g8" rel="noopener" class="external-link" href="https://read.readwise.io/read/01hdpaprp603dpr2ngjev5a4g8" target="_blank">View Highlight</a>)</li><li data-line="26">The <a data-tooltip-position="top" aria-label="https://arxiv.org/abs/2306.05685" rel="noopener" class="external-link" href="https://arxiv.org/abs/2306.05685" target="_blank">Vicuna</a> paper adopted a similar approach. (<a data-tooltip-position="top" aria-label="https://read.readwise.io/read/01hdpar4ggfr2296ckscvthtqx" rel="noopener" class="external-link" href="https://read.readwise.io/read/01hdpar4ggfr2296ckscvthtqx" target="_blank">View Highlight</a>)</li><li data-line="27"><a data-tooltip-position="top" aria-label="https://arxiv.org/abs/2305.14314" rel="noopener" class="external-link" href="https://arxiv.org/abs/2305.14314" target="_blank">QLoRA</a> also used an LLM to evaluate another LLM’s output. (<a data-tooltip-position="top" aria-label="https://read.readwise.io/read/01hdpasx29at5q5wx9v3r1hcb3" rel="noopener" class="external-link" href="https://read.readwise.io/read/01hdpasx29at5q5wx9v3r1hcb3" target="_blank">View Highlight</a>)</li><li data-line="28">And if our task has no correct answer but we have references (e.g., machine translation, extractive summarization), we might have to rely on lossier reference metrics based on matching (BLEU, ROUGE) or semantic similarity (BERTScore, MoverScore). (<a data-tooltip-position="top" aria-label="https://read.readwise.io/read/01hdpb99z7ef5v5px6yb4becw5" rel="noopener" class="external-link" href="https://read.readwise.io/read/01hdpb99z7ef5v5px6yb4becw5" target="_blank">View Highlight</a>)</li><li data-line="29">we may opt to lean on <strong>automated evaluations via a strong LLM</strong>. (<a data-tooltip-position="top" aria-label="https://read.readwise.io/read/01hdpb9jsgemdr1aqdyxbeq7wy" rel="noopener" class="external-link" href="https://read.readwise.io/read/01hdpb9jsgemdr1aqdyxbeq7wy" target="_blank">View Highlight</a>)</li><li data-line="30">Thus, instead of using off-the-shelf benchmarks, we can <strong>start by collecting a set of task-specific evals</strong> (i.e., prompt, context, expected outputs as references). These evals will then guide prompt engineering, model selection, fine-tuning, and so on. And as we tweak the system, we can run these evals to quickly measure improvements or regressions. Think of it as Eval Driven Development (EDD). (<a data-tooltip-position="top" aria-label="https://read.readwise.io/read/01h9v5284xf2t7y0rs7n2rar61" rel="noopener" class="external-link" href="https://read.readwise.io/read/01h9v5284xf2t7y0rs7n2rar61" target="_blank">View Highlight</a>)<ul><li data-line="31">Note: <a data-href="Eval Driven Development" href="Eval Driven Development" class="internal-link is-unresolved" target="_self" rel="noopener">Eval Driven Development</a> (EDD)</li></ul></li><li data-line="32">Position bias: LLMs like GPT-4 tend to favor the response in the first position. To mitigate this, we can evaluate the same pair of responses twice while swapping their order. If the same response is preferred in both orders, we mark it as a win. Otherwise, it’s a tie. (<a data-tooltip-position="top" aria-label="https://read.readwise.io/read/01h9v58y78vam6qsqe1d0d9w02" rel="noopener" class="external-link" href="https://read.readwise.io/read/01h9v58y78vam6qsqe1d0d9w02" target="_blank">View Highlight</a>)</li><li data-line="33">Verbosity bias: LLMs tend to favor longer, wordier responses over more concise ones, even if the latter is clearer and of higher quality. A possible solution is to ensure that comparison responses are similar in length. (<a data-tooltip-position="top" aria-label="https://read.readwise.io/read/01h9v5946f562k03ja4a5h35wz" rel="noopener" class="external-link" href="https://read.readwise.io/read/01h9v5946f562k03ja4a5h35wz" target="_blank">View Highlight</a>)</li><li data-line="34">RAG helps reduce hallucination by grounding the model on the retrieved context, thus increasing factuality. (<a data-tooltip-position="top" aria-label="https://read.readwise.io/read/01hdpbpmsjkjg16tg4n6f5z0zm" rel="noopener" class="external-link" href="https://read.readwise.io/read/01hdpbpmsjkjg16tg4n6f5z0zm" target="_blank">View Highlight</a>)</li><li data-line="35">Self-enhancement bias: LLMs have a slight bias towards their own answers. GPT-4 favors itself with a 10% higher win rate while Claude-v1 favors itself with a 25% higher win rate. To counter this, don’t use the same LLM for evaluation tasks. (<a data-tooltip-position="top" aria-label="https://read.readwise.io/read/01h9v5bpvdb9453g6gamazrd61" rel="noopener" class="external-link" href="https://read.readwise.io/read/01h9v5bpvdb9453g6gamazrd61" target="_blank">View Highlight</a>)</li><li data-line="36">Another tip: Rather than asking an LLM for a direct evaluation (via giving a score), try giving it a reference and asking for a comparison. This helps with reducing noise. (<a data-tooltip-position="top" aria-label="https://read.readwise.io/read/01h9v5c3b40qehvgc7c53mg4rb" rel="noopener" class="external-link" href="https://read.readwise.io/read/01h9v5c3b40qehvgc7c53mg4rb" target="_blank">View Highlight</a>)</li><li data-line="37">The vibe-based eval cannot be underrated. … One of our evals was just having a bunch of prompts and watching the answers as the models trained and see if they change. Honestly, I don’t really believe that any of these eval metrics capture what we care about. One of our prompts was “suggest games for a 3-year-old and a 7-year-old to play” and that was a lot more valuable to see how the answer changed during the course of training. (<a data-tooltip-position="top" aria-label="https://read.readwise.io/read/01h9v5dq23ey7xwv6996qsk9ma" rel="noopener" class="external-link" href="https://read.readwise.io/read/01h9v5dq23ey7xwv6996qsk9ma" target="_blank">View Highlight</a>)</li><li data-line="38">Retrieval-Augmented Generation (RAG) fetches relevant data from outside the foundation model and enhances the input with this data, providing richer context to improve output. (<a data-tooltip-position="top" aria-label="https://read.readwise.io/read/01h9v5eva2rbhgbnr4j8hhx9xe" rel="noopener" class="external-link" href="https://read.readwise.io/read/01h9v5eva2rbhgbnr4j8hhx9xe" target="_blank">View Highlight</a>)<ul><li data-line="39">Note: <a data-href="Retrieval-Augmented Generation" href="Retrieval-Augmented Generation" class="internal-link is-unresolved" target="_self" rel="noopener">Retrieval-Augmented Generation</a> (RAG)</li></ul></li><li data-line="40">Huggingface’s <a data-tooltip-position="top" aria-label="https://huggingface.co/spaces/mteb/leaderboard" rel="noopener" class="external-link" href="https://huggingface.co/spaces/mteb/leaderboard" target="_blank">Massive Text Embedding Benchmark (MTEB)</a> scores various models on diverse tasks such as classification, clustering, retrieval, summarization, etc. (<a data-tooltip-position="top" aria-label="https://read.readwise.io/read/01h9v5nh54qveeheh6kz0fr4j1" rel="noopener" class="external-link" href="https://read.readwise.io/read/01h9v5nh54qveeheh6kz0fr4j1" target="_blank">View Highlight</a>)<ul><li data-line="41">Note: <a data-href="Huggingface" href="Huggingface" class="internal-link is-unresolved" target="_self" rel="noopener">Huggingface</a></li></ul></li><li data-line="42">For example, <a data-tooltip-position="top" aria-label="https://arxiv.org/abs/2103.00020" rel="noopener" class="external-link" href="https://arxiv.org/abs/2103.00020" target="_blank">CLIP</a> is multimodal and embeds images and text in the same space, allowing us to find images most similar to an input text. (<a data-tooltip-position="top" aria-label="https://read.readwise.io/read/01h9v5rf3dampkc38qqz47vfvs" rel="noopener" class="external-link" href="https://read.readwise.io/read/01h9v5rf3dampkc38qqz47vfvs" target="_blank">View Highlight</a>)<ul><li data-line="43">Note: <a data-href="CLIP" href="CLIP" class="internal-link is-unresolved" target="_self" rel="noopener">CLIP</a> is multimodal embedding</li></ul></li><li data-line="44"><strong><a data-tooltip-position="top" aria-label="https://arxiv.org/abs/2005.11401" rel="noopener" class="external-link" href="https://arxiv.org/abs/2005.11401" target="_blank">Retrieval Augmented Generation (RAG)</a></strong>, from which this pattern gets its name, highlighted the downsides of pre-trained LLMs. These include not being able to expand or revise memory, not providing insights into generated output, and hallucinations. (<a data-tooltip-position="top" aria-label="https://read.readwise.io/read/01hdpc2gdbd581wxvv4992hzsz" rel="noopener" class="external-link" href="https://read.readwise.io/read/01hdpc2gdbd581wxvv4992hzsz" target="_blank">View Highlight</a>)</li><li data-line="45">Overview of Retrieval Augmented Generation (<a data-tooltip-position="top" aria-label="https://arxiv.org/abs/2005.11401" rel="noopener" class="external-link" href="https://arxiv.org/abs/2005.11401" target="_blank">source</a>)<br>During inference, they concatenate the input with the retrieved document. Then, the LLM generates tokeni based on the original input, the retrieved document, and the previous i−1 tokens. (<a data-tooltip-position="top" aria-label="https://read.readwise.io/read/01ha04p6rtdtxgthry4j44f6mm" rel="noopener" class="external-link" href="https://read.readwise.io/read/01ha04p6rtdtxgthry4j44f6mm" target="_blank">View Highlight</a>)</li><li data-line="47"><a data-tooltip-position="top" aria-label="https://arxiv.org/abs/2112.04426" rel="noopener" class="external-link" href="https://arxiv.org/abs/2112.04426" target="_blank"><strong>Retrieval-Enhanced Transformer (RETRO)</strong></a> adopts a similar pattern where it combines a frozen BERT retriever, a differentiable encoder, and chunked cross-attention to generate output. What’s different is that RETRO does retrieval throughout the entire pre-training stage, and not just during inference. Furthermore, they fetch relevant documents based on chunks of the input. This allows for finer-grained, repeated retrieval during generation instead of only retrieving once per query. (<a data-tooltip-position="top" aria-label="https://read.readwise.io/read/01ha04v0pn9sh5ggfvawyn9y2g" rel="noopener" class="external-link" href="https://read.readwise.io/read/01ha04v0pn9sh5ggfvawyn9y2g" target="_blank">View Highlight</a>)</li><li data-line="48"><strong><a data-tooltip-position="top" aria-label="https://arxiv.org/abs/2203.05115" rel="noopener" class="external-link" href="https://arxiv.org/abs/2203.05115" target="_blank">Internet-augmented LMs</a></strong> proposes using a humble “off-the-shelf” search engine to augment LLMs. First, they retrieve a set of relevant documents via Google Search. Since these retrieved documents tend to be long (average length 2,056 words), they chunk them into paragraphs of six sentences each. Finally, they embed the question and paragraphs via TF-IDF and applied cosine similarity to rank the most relevant paragraphs for each query. (<a data-tooltip-position="top" aria-label="https://read.readwise.io/read/01ha04vp8d8af16hyw5te6gyhj" rel="noopener" class="external-link" href="https://read.readwise.io/read/01ha04vp8d8af16hyw5te6gyhj" target="_blank">View Highlight</a>)</li><li data-line="49"><strong>What if we don’t have relevance judgments for query-passage pairs?</strong> Without them, we would not be able to train the bi-encoders that embed the queries and documents in the same embedding space, where relevance is represented by the inner product. <strong><a data-tooltip-position="top" aria-label="https://arxiv.org/abs/2212.10496" rel="noopener" class="external-link" href="https://arxiv.org/abs/2212.10496" target="_blank">Hypothetical document embeddings (HyDE)</a></strong> suggests a solution. (<a data-tooltip-position="top" aria-label="https://read.readwise.io/read/01hdpcsjn3zaf6xtsb5mkwmsr4" rel="noopener" class="external-link" href="https://read.readwise.io/read/01hdpcsjn3zaf6xtsb5mkwmsr4" target="_blank">View Highlight</a>)</li><li data-line="50">From <a data-tooltip-position="top" aria-label="https://eugeneyan.com/writing/obsidian-copilot/" rel="noopener" class="external-link" href="https://eugeneyan.com/writing/obsidian-copilot/" target="_blank">personal experience</a>, I’ve found that hybrid retrieval (traditional search index + embedding-based search) works better than either alone. (<a data-tooltip-position="top" aria-label="https://read.readwise.io/read/01hdpcwnsbccsxqxg1sk0yrtgd" rel="noopener" class="external-link" href="https://read.readwise.io/read/01hdpcwnsbccsxqxg1sk0yrtgd" target="_blank">View Highlight</a>)</li><li data-line="51"><strong>With regard to embeddings</strong>, the seemingly popular approach is to use <a data-tooltip-position="top" aria-label="https://openai.com/blog/new-and-improved-embedding-model" rel="noopener" class="external-link" href="https://openai.com/blog/new-and-improved-embedding-model" target="_blank"><code>text-embedding-ada-002</code></a>. Its benefits include ease of use via an API and not having to maintain our own embedding infra or self-host embedding models. (<a data-tooltip-position="top" aria-label="https://read.readwise.io/read/01hdpd0tbw4nxmkcyj0ptqs4mr" rel="noopener" class="external-link" href="https://read.readwise.io/read/01hdpd0tbw4nxmkcyj0ptqs4mr" target="_blank">View Highlight</a>)</li><li data-line="52">In addition, with a conventional search index, we can use metadata to refine results. For example, we can use date filters to prioritize newer documents or narrow our search to a specific time period. (<a data-tooltip-position="top" aria-label="https://read.readwise.io/read/01ha04yy0q5zj8s203ssq2bj4z" rel="noopener" class="external-link" href="https://read.readwise.io/read/01ha04yy0q5zj8s203ssq2bj4z" target="_blank">View Highlight</a>)</li><li data-line="53">The OG embedding approaches include Word2vec and <a data-tooltip-position="top" aria-label="https://fasttext.cc" rel="noopener" class="external-link" href="https://fasttext.cc" target="_blank">fastText</a>. FastText is an open-source, lightweight library that enables users to leverage pre-trained embeddings or train new embedding models. It comes with pre-trained embeddings for 157 languages and is extremely fast, even without a GPU. It’s my go-to for early-stage proof of concepts. (<a data-tooltip-position="top" aria-label="https://read.readwise.io/read/01ha050772yc30tk42k2ybn5rd" rel="noopener" class="external-link" href="https://read.readwise.io/read/01ha050772yc30tk42k2ybn5rd" target="_blank">View Highlight</a>)<ul><li data-line="54">Note: <a data-href="FastText" href="FastText" class="internal-link is-unresolved" target="_self" rel="noopener">FastText</a></li></ul></li><li data-line="55">Another good baseline is <a data-tooltip-position="top" aria-label="https://github.com/UKPLab/sentence-transformers" rel="noopener" class="external-link" href="https://github.com/UKPLab/sentence-transformers" target="_blank">sentence-transformers</a>. It makes it simple to compute embeddings for sentences, paragraphs, and even images. It’s based on workhorse transformers such as BERT and RoBERTa and is available in more than 100 languages. (<a data-tooltip-position="top" aria-label="https://read.readwise.io/read/01ha050qrmwsax2ycmn6f4rjsw" rel="noopener" class="external-link" href="https://read.readwise.io/read/01ha050qrmwsax2ycmn6f4rjsw" target="_blank">View Highlight</a>)</li><li data-line="56"><a data-tooltip-position="top" aria-label="https://en.wikipedia.org/wiki/Locality-sensitive_hashing" rel="noopener" class="external-link" href="https://en.wikipedia.org/wiki/Locality-sensitive_hashing" target="_blank">Locality Sensitive Hashing</a> (LSH): The core idea is to create hash functions such that similar items are likely to end up in the same hash bucket. (<a data-tooltip-position="top" aria-label="https://read.readwise.io/read/01hdpdcjcv70a7z7d04kjevay1" rel="noopener" class="external-link" href="https://read.readwise.io/read/01hdpdcjcv70a7z7d04kjevay1" target="_blank">View Highlight</a>)</li><li data-line="57"><a data-tooltip-position="top" aria-label="https://github.com/facebookresearch/faiss" rel="noopener" class="external-link" href="https://github.com/facebookresearch/faiss" target="_blank">Facebook AI Similarity Search</a> (FAISS): It uses a combination of quantization and indexing for efficient retrieval, supports both CPU and GPU, and can handle billions of vectors due to its efficient use of memory. (<a data-tooltip-position="top" aria-label="https://read.readwise.io/read/01hdpdcqryxrmkpj4fvsgejpec" rel="noopener" class="external-link" href="https://read.readwise.io/read/01hdpdcqryxrmkpj4fvsgejpec" target="_blank">View Highlight</a>)</li><li data-line="58"><a data-tooltip-position="top" aria-label="https://github.com/nmslib/hnswlib" rel="noopener" class="external-link" href="https://github.com/nmslib/hnswlib" target="_blank">Hierarchical Navigable Small Worlds</a> (HNSW): Inspired by “six degrees of separation”, it builds a hierarchical graph structure that embodies the small world phenomenon. (<a data-tooltip-position="top" aria-label="https://read.readwise.io/read/01hdpdcx6sfgwmxfy0bkwst9sn" rel="noopener" class="external-link" href="https://read.readwise.io/read/01hdpdcx6sfgwmxfy0bkwst9sn" target="_blank">View Highlight</a>)</li><li data-line="59"><a data-tooltip-position="top" aria-label="https://github.com/google-research/google-research/tree/master/scann" rel="noopener" class="external-link" href="https://github.com/google-research/google-research/tree/master/scann" target="_blank">Scalable Nearest Neighbors</a> (ScaNN): ANN is done via a two-step process. (<a data-tooltip-position="top" aria-label="https://read.readwise.io/read/01hdpdd19fy5nsb5mvrf5m9p7k" rel="noopener" class="external-link" href="https://read.readwise.io/read/01hdpdd19fy5nsb5mvrf5m9p7k" target="_blank">View Highlight</a>)</li><li data-line="60">Some popular techniques include: (<a data-tooltip-position="top" aria-label="https://read.readwise.io/read/01ha05500k0g9rpg9yenhmmazt" rel="noopener" class="external-link" href="https://read.readwise.io/read/01ha05500k0g9rpg9yenhmmazt" target="_blank">View Highlight</a>)<ul><li data-line="61">Note: Similarity search techniques</li></ul></li><li data-line="62">No single framework is better than all others in every aspect. Thus, start by defining your functional and non-functional requirements before benchmarking. (<a data-tooltip-position="top" aria-label="https://read.readwise.io/read/01hdpde234cnj6v5vcf817zdws" rel="noopener" class="external-link" href="https://read.readwise.io/read/01hdpde234cnj6v5vcf817zdws" target="_blank">View Highlight</a>)</li><li data-line="63"><strong>Continued pre-training</strong>: With domain-specific data, apply the same pre-training regime (next token prediction, masked language modeling) on the base model. (<a data-tooltip-position="top" aria-label="https://read.readwise.io/read/01hdpdjafw6yxn7kjj3dcd1xq9" rel="noopener" class="external-link" href="https://read.readwise.io/read/01hdpdjafw6yxn7kjj3dcd1xq9" target="_blank">View Highlight</a>)</li><li data-line="64"><strong>Instruction fine-tuning</strong>: The pre-trained (base) model is fine-tuned on examples of instruction-output pairs to follow instructions, answer questions, be waifu, etc. (<a data-tooltip-position="top" aria-label="https://read.readwise.io/read/01hdpdjes1zky356anmx9y3sye" rel="noopener" class="external-link" href="https://read.readwise.io/read/01hdpdjes1zky356anmx9y3sye" target="_blank">View Highlight</a>)</li><li data-line="65"><strong>Single-task fine-tuning</strong>: The pre-trained model is honed for a narrow and specific task such as toxicity detection or summarization, similar to BERT and T5. (<a data-tooltip-position="top" aria-label="https://read.readwise.io/read/01hdpdjkm4j51v80zzqrgzmt8f" rel="noopener" class="external-link" href="https://read.readwise.io/read/01hdpdjkm4j51v80zzqrgzmt8f" target="_blank">View Highlight</a>)</li><li data-line="66"><strong>Reinforcement learning with human feedback (RLHF)</strong>: This combines instruction fine-tuning with reinforcement learning. It requires collecting human preferences (e.g., pairwise comparisons) which are then used to train a reward model. (<a data-tooltip-position="top" aria-label="https://read.readwise.io/read/01hdpdjzh9k94qmwfr23e5ab0h" rel="noopener" class="external-link" href="https://read.readwise.io/read/01hdpdjzh9k94qmwfr23e5ab0h" target="_blank">View Highlight</a>)</li><li data-line="67">However, fine-tuning isn’t without its challenges. First, we <strong>need a significant volume of demonstration data</strong>. For instance, in the <a data-tooltip-position="top" aria-label="https://arxiv.org/abs/2203.02155" rel="noopener" class="external-link" href="https://arxiv.org/abs/2203.02155" target="_blank">InstructGPT paper</a>, they used 13k instruction-output samples for supervised fine-tuning, 33k output comparisons for reward modeling, and 31k prompts without human labels as input for RLHF. (<a data-tooltip-position="top" aria-label="https://read.readwise.io/read/01ha0g8w5td7h5a11hpnrk2818" rel="noopener" class="external-link" href="https://read.readwise.io/read/01ha0g8w5td7h5a11hpnrk2818" target="_blank">View Highlight</a>)</li><li data-line="68"><strong><a data-tooltip-position="top" aria-label="https://arxiv.org/abs/2104.08691" rel="noopener" class="external-link" href="https://arxiv.org/abs/2104.08691" target="_blank">Soft prompt tuning</a></strong> prepends a trainable tensor to the model’s input embeddings, essentially creating a soft prompt. Unlike discrete text prompts, soft prompts can be learned via backpropagation, meaning they can be fine-tuned to incorporate signals from any number of labeled examples. (<a data-tooltip-position="top" aria-label="https://read.readwise.io/read/01ha0hdzbn1qy3zgs50xrhezs9" rel="noopener" class="external-link" href="https://read.readwise.io/read/01ha0hdzbn1qy3zgs50xrhezs9" target="_blank">View Highlight</a>)</li><li data-line="69">Next, there’s <strong><a data-tooltip-position="top" aria-label="https://arxiv.org/abs/2101.00190" rel="noopener" class="external-link" href="https://arxiv.org/abs/2101.00190" target="_blank">prefix tuning</a></strong>. Instead of adding a soft prompt to the model input, it prepends trainable parameters to the hidden states of all transformer blocks. During fine-tuning, the LM’s original parameters are kept frozen while the prefix parameters are updated. (<a data-tooltip-position="top" aria-label="https://read.readwise.io/read/01ha0he3wjjtpcjq0b4777hgt1" rel="noopener" class="external-link" href="https://read.readwise.io/read/01ha0he3wjjtpcjq0b4777hgt1" target="_blank">View Highlight</a>)</li><li data-line="70"><strong><a data-tooltip-position="top" aria-label="https://arxiv.org/abs/2305.14314" rel="noopener" class="external-link" href="https://arxiv.org/abs/2305.14314" target="_blank">QLoRA</a></strong> builds on the idea of LoRA. But instead of using the full 16-bit model during fine-tuning, it applies a 4-bit quantized model. (<a data-tooltip-position="top" aria-label="https://read.readwise.io/read/01hdpedktpznj3kc6556e1dd4c" rel="noopener" class="external-link" href="https://read.readwise.io/read/01hdpedktpznj3kc6556e1dd4c" target="_blank">View Highlight</a>)</li><li data-line="71">How to apply fine-tuning? (<a data-tooltip-position="top" aria-label="https://read.readwise.io/read/01hdpee1aqtphapa054mqb9xj0" rel="noopener" class="external-link" href="https://read.readwise.io/read/01hdpee1aqtphapa054mqb9xj0" target="_blank">View Highlight</a>)</li><li data-line="72">Via experts or crowd-sourced human annotators (<a data-tooltip-position="top" aria-label="https://read.readwise.io/read/01hdpeewpm2ggxt323rz2arp7x" rel="noopener" class="external-link" href="https://read.readwise.io/read/01hdpeewpm2ggxt323rz2arp7x" target="_blank">View Highlight</a>)</li><li data-line="73">Via user feedback (<a data-tooltip-position="top" aria-label="https://read.readwise.io/read/01hdpeeykzbmbbf72faw12nrk7" rel="noopener" class="external-link" href="https://read.readwise.io/read/01hdpeeykzbmbbf72faw12nrk7" target="_blank">View Highlight</a>)</li><li data-line="74">Query larger open models with permissive licenses (<a data-tooltip-position="top" aria-label="https://read.readwise.io/read/01hdpef1chykfv7zha9sxrfap4" rel="noopener" class="external-link" href="https://read.readwise.io/read/01hdpef1chykfv7zha9sxrfap4" target="_blank">View Highlight</a>)</li><li data-line="75">Reuse open-source data (<a data-tooltip-position="top" aria-label="https://read.readwise.io/read/01hdpef44saeq4bda16s74pm5t" rel="noopener" class="external-link" href="https://read.readwise.io/read/01hdpef44saeq4bda16s74pm5t" target="_blank">View Highlight</a>)</li><li data-line="76">When a new request is received:<br>• Embedding generator: This embeds the request via various models such as OpenAI’s <code>text-embedding-ada-002</code>, FastText, Sentence Transformers, and more.<br>• Similarity evaluator: This computes the similarity of the request via the vector store and then provides a distance metric. The vector store can either be local (FAISS, Hnswlib) or cloud-based. It can also compute similarity via a model.<br>• Cache storage: If the request is similar, the cached response is fetched and served.<br>• LLM: If the request isn’t similar enough, it gets passed to the LLM which then generates the result. Finally, the response is served and cached for future use. (<a data-tooltip-position="top" aria-label="https://read.readwise.io/read/01hdpesaznvbm53hmrmmt0stfe" rel="noopener" class="external-link" href="https://read.readwise.io/read/01hdpesaznvbm53hmrmmt0stfe" target="_blank">View Highlight</a>)</li><li data-line="81">In the space of serving LLM generations, the popularized approach is to cache the LLM response keyed on the embedding of the input request. Then, for each new request, if a semantically similar request is received, we can serve the cached response. (<a data-tooltip-position="top" aria-label="https://read.readwise.io/read/01ha0htpjjqgxb0qm9nb3pwcwf" rel="noopener" class="external-link" href="https://read.readwise.io/read/01ha0htpjjqgxb0qm9nb3pwcwf" target="_blank">View Highlight</a>)</li><li data-line="82">For some practitioners, this sounds like “<a data-tooltip-position="top" aria-label="https://twitter.com/HanchungLee/status/1681146845186363392" rel="noopener" class="external-link" href="https://twitter.com/HanchungLee/status/1681146845186363392" target="_blank">a disaster waiting to happen.</a>” I’m inclined to agree. Thus, I think the key to adopting this pattern is figuring out how to cache safely, instead of solely depending on semantic similarity. (<a data-tooltip-position="top" aria-label="https://read.readwise.io/read/01ha0hvj8qjj8p1cy431kk2445" rel="noopener" class="external-link" href="https://read.readwise.io/read/01ha0hvj8qjj8p1cy431kk2445" target="_blank">View Highlight</a>)</li><li data-line="83">An example of caching for LLMs is <a data-tooltip-position="top" aria-label="https://github.com/zilliztech/GPTCache" rel="noopener" class="external-link" href="https://github.com/zilliztech/GPTCache" target="_blank">GPTCache</a>. (<a data-tooltip-position="top" aria-label="https://read.readwise.io/read/01ha0hx15qmhqq0ykxq2r91ct7" rel="noopener" class="external-link" href="https://read.readwise.io/read/01ha0hx15qmhqq0ykxq2r91ct7" target="_blank">View Highlight</a>)</li><li data-line="84">One way to quantify this is via the cache hit rate (percentage of requests served directly from the cache). If the usage pattern is uniformly random, the cache would need frequent updates. Thus, the effort to keep the cache up-to-date could negate any benefit a cache has to offer. On the other hand, if the usage follows a power law where a small proportion of unique requests account for the majority of traffic (e.g., search queries, product views), then caching could be an effective strategy. (<a data-tooltip-position="top" aria-label="https://read.readwise.io/read/01hdpf1aaem31vjezrrscayz1f" rel="noopener" class="external-link" href="https://read.readwise.io/read/01hdpf1aaem31vjezrrscayz1f" target="_blank">View Highlight</a>)</li><li data-line="85">Beyond semantic similarity, we could also explore caching based on: (<a data-tooltip-position="top" aria-label="https://read.readwise.io/read/01hdpf2g0j8xn7t1kerf5kyw3f" rel="noopener" class="external-link" href="https://read.readwise.io/read/01hdpf2g0j8xn7t1kerf5kyw3f" target="_blank">View Highlight</a>)</li><li data-line="86">Now, bringing it back to LLM responses. Imagine we get a request for a summary of “Mission Impossible 2” that’s semantically similar enough to “Mission Impossible 3”. If we’re looking up cache based on semantic similarity, we could serve the wrong response. (<a data-tooltip-position="top" aria-label="https://read.readwise.io/read/01ha0j9ahyebcvqrqdar63b5mg" rel="noopener" class="external-link" href="https://read.readwise.io/read/01ha0j9ahyebcvqrqdar63b5mg" target="_blank">View Highlight</a>)</li><li data-line="87">They ensure that model outputs are reliable and consistent enough to use in production. (<a data-tooltip-position="top" aria-label="https://read.readwise.io/read/01hdpfaptsrp39qej0zx62pvmw" rel="noopener" class="external-link" href="https://read.readwise.io/read/01hdpfaptsrp39qej0zx62pvmw" target="_blank">View Highlight</a>)</li><li data-line="88">Guardrails: To ensure output quality (<a data-tooltip-position="top" aria-label="https://read.readwise.io/read/01ha0jdgezjzph60w368ywba9m" rel="noopener" class="external-link" href="https://read.readwise.io/read/01ha0jdgezjzph60w368ywba9m" target="_blank">View Highlight</a>)</li><li data-line="89">Broadly speaking, the guardrails fall into the following categories: (<a data-tooltip-position="top" aria-label="https://read.readwise.io/read/01hdpfe2jcq537c4d940at51s7" rel="noopener" class="external-link" href="https://read.readwise.io/read/01hdpfe2jcq537c4d940at51s7" target="_blank">View Highlight</a>)</li><li data-line="90">Single output value validation: This includes ensuring that the output (i) is one of the predefined choices, (ii) has a length within a certain range, (iii) if numeric, falls within an expected range, and (iv) is a complete sentence. (<a data-tooltip-position="top" aria-label="https://read.readwise.io/read/01hdpfe8se2xf3sz8q9x0exmsd" rel="noopener" class="external-link" href="https://read.readwise.io/read/01hdpfe8se2xf3sz8q9x0exmsd" target="_blank">View Highlight</a>)</li><li data-line="91">Syntactic checks: This includes ensuring that generated URLs are valid and reachable, and that Python and SQL code is bug-free. (<a data-tooltip-position="top" aria-label="https://read.readwise.io/read/01hdpfead7mv4bcadytwnqzcgb" rel="noopener" class="external-link" href="https://read.readwise.io/read/01hdpfead7mv4bcadytwnqzcgb" target="_blank">View Highlight</a>)</li><li data-line="92">Semantic checks: This verifies that the output is aligned with the reference document, or that the extractive summary closely matches the source document. These checks can be done via cosine similarity or fuzzy matching techniques. (<a data-tooltip-position="top" aria-label="https://read.readwise.io/read/01hdpfeg1qdwrd0bf1m51gk86d" rel="noopener" class="external-link" href="https://read.readwise.io/read/01hdpfeg1qdwrd0bf1m51gk86d" target="_blank">View Highlight</a>)</li><li data-line="93"><strong>One approach is to control the model’s responses via prompts.</strong> For example, Anthropic shared about prompts designed to guide the model toward generating responses that are <a data-tooltip-position="top" aria-label="https://arxiv.org/abs/2204.05862" rel="noopener" class="external-link" href="https://arxiv.org/abs/2204.05862" target="_blank">helpful, harmless, and honest</a> (HHH). (<a data-tooltip-position="top" aria-label="https://read.readwise.io/read/01ha0jnkta96efzmsjm1pnyaap" rel="noopener" class="external-link" href="https://read.readwise.io/read/01ha0jnkta96efzmsjm1pnyaap" target="_blank">View Highlight</a>)<ul><li data-line="94">Note: Paper about designing prompts to guide the model towards generating harmless responses.</li></ul></li><li data-line="95">Safety checks: This ensures that the generated output is free of inappropriate language or that the quality of translated text is high. (<a data-tooltip-position="top" aria-label="https://read.readwise.io/read/01hdpfejn4neh7r00c6q4ayxxw" rel="noopener" class="external-link" href="https://read.readwise.io/read/01hdpfejn4neh7r00c6q4ayxxw" target="_blank">View Highlight</a>)</li><li data-line="96">Nvidia’s <a data-tooltip-position="top" aria-label="https://github.com/NVIDIA/NeMo-Guardrails" rel="noopener" class="external-link" href="https://github.com/NVIDIA/NeMo-Guardrails" target="_blank">NeMo-Guardrails</a> follows a similar principle but is designed to guide LLM-based conversational systems. Rather than focusing on syntactic guardrails, it emphasizes semantic ones. This includes ensuring that the assistant steers clear of politically charged topics, provides factually correct information, and can detect jailbreaking attempts. (<a data-tooltip-position="top" aria-label="https://read.readwise.io/read/01ha0jvycjcshrz65xm1ndttam" rel="noopener" class="external-link" href="https://read.readwise.io/read/01ha0jvycjcshrz65xm1ndttam" target="_blank">View Highlight</a>)</li><li data-line="97">Thus, NeMo’s approach is somewhat different: Instead of using more deterministic checks like verifying if a value exists in a list or inspecting code for bugs, NeMo leans heavily on using another LLM to validate outputs (inspired by <a data-tooltip-position="top" aria-label="https://arxiv.org/abs/2303.08896" rel="noopener" class="external-link" href="https://arxiv.org/abs/2303.08896" target="_blank">SelfCheckGPT</a>) (<a data-tooltip-position="top" aria-label="https://read.readwise.io/read/01ha0jww3n6gtvma3kkdbeqss7" rel="noopener" class="external-link" href="https://read.readwise.io/read/01ha0jww3n6gtvma3kkdbeqss7" target="_blank">View Highlight</a>)<ul><li data-line="98">Note: <a data-href="SelfCheckGPT" href="SelfCheckGPT" class="internal-link is-unresolved" target="_self" rel="noopener">SelfCheckGPT</a></li></ul></li><li data-line="99">Apart from using guardrails to verify the output of LLMs, we can also <strong>directly steer the output to adhere to a specific grammar.</strong> An example of this is Microsoft’s <a data-tooltip-position="top" aria-label="https://github.com/microsoft/guidance" rel="noopener" class="external-link" href="https://github.com/microsoft/guidance" target="_blank">Guidance</a>. (<a data-tooltip-position="top" aria-label="https://read.readwise.io/read/01ha0k2ar09y9nryjejvsvdvg8" rel="noopener" class="external-link" href="https://read.readwise.io/read/01ha0k2ar09y9nryjejvsvdvg8" target="_blank">View Highlight</a>)</li><li data-line="100">Structural guidance (<a data-tooltip-position="top" aria-label="https://read.readwise.io/read/01hdpfvn36s08411tc4sr13gmj" rel="noopener" class="external-link" href="https://read.readwise.io/read/01hdpfvn36s08411tc4sr13gmj" target="_blank">View Highlight</a>)</li><li data-line="101">Syntactic guardrails (<a data-tooltip-position="top" aria-label="https://read.readwise.io/read/01hdpfvr2xzt3c9j29k9483gza" rel="noopener" class="external-link" href="https://read.readwise.io/read/01hdpfvr2xzt3c9j29k9483gza" target="_blank">View Highlight</a>)</li><li data-line="102">Content safety guardrails (<a data-tooltip-position="top" aria-label="https://read.readwise.io/read/01hdpfvtc7j4bjmfz0784vm5xc" rel="noopener" class="external-link" href="https://read.readwise.io/read/01hdpfvtc7j4bjmfz0784vm5xc" target="_blank">View Highlight</a>)</li><li data-line="103">Semantic/factuality guardrails (<a data-tooltip-position="top" aria-label="https://read.readwise.io/read/01hdpfvzhhv3byqkcp493hzmwz" rel="noopener" class="external-link" href="https://read.readwise.io/read/01hdpfvzhhv3byqkcp493hzmwz" target="_blank">View Highlight</a>)</li><li data-line="104">We can validate if the produced summary is semantically similar to the output, or have (another) LLM ascertain if the summary accurately represents the provided synopsis. (<a data-tooltip-position="top" aria-label="https://read.readwise.io/read/01hdpg2xz43sfsgj2n2g4tg0tb" rel="noopener" class="external-link" href="https://read.readwise.io/read/01hdpg2xz43sfsgj2n2g4tg0tb" target="_blank">View Highlight</a>)</li><li data-line="105">Input guardrails (<a data-tooltip-position="top" aria-label="https://read.readwise.io/read/01hdpfwaepqremn0178ff7qn3j" rel="noopener" class="external-link" href="https://read.readwise.io/read/01hdpfwaepqremn0178ff7qn3j" target="_blank">View Highlight</a>)</li><li data-line="106">Defensive UX: To anticipate &amp; handle errors gracefully (<a data-tooltip-position="top" aria-label="https://read.readwise.io/read/01ha0kegjvrtpy0ynyvwb0aw5c" rel="noopener" class="external-link" href="https://read.readwise.io/read/01ha0kegjvrtpy0ynyvwb0aw5c" target="_blank">View Highlight</a>)</li><li data-line="107"><strong>Microsoft’s <a data-tooltip-position="top" aria-label="https://www.microsoft.com/en-us/research/publication/guidelines-for-human-ai-interaction/" rel="noopener" class="external-link" href="https://www.microsoft.com/en-us/research/publication/guidelines-for-human-ai-interaction/" target="_blank">Guidelines for Human-AI Interaction</a></strong> is based on a survey of 168 potential guidelines, collected from internal and external industry sources, academic literature, and public articles. After clustering guidelines that were similar, filtering guidelines that were too vague or too specific or not AI-specific, and a round of heuristic evaluation, they narrowed it down to 18 guidelines. (<a data-tooltip-position="top" aria-label="https://read.readwise.io/read/01ha0kgp3m6r6qr1ehbcjr4t9h" rel="noopener" class="external-link" href="https://read.readwise.io/read/01ha0kgp3m6r6qr1ehbcjr4t9h" target="_blank">View Highlight</a>)</li><li data-line="108">• Initially: Make clear what the system can do (G1), make clear how well the system can do what it can do (G2)<br>• During interaction: Time services based on context (G3), mitigate social biases (G6)<br>• When wrong: Support efficient dismissal (G8), support efficient correction (G9)<br>• Over time: Learn from user behavior (G13), provide global controls (G17) (<a data-tooltip-position="top" aria-label="https://read.readwise.io/read/01ha0kjaaya3dmdtqmcrvfr2nz" rel="noopener" class="external-link" href="https://read.readwise.io/read/01ha0kjaaya3dmdtqmcrvfr2nz" target="_blank">View Highlight</a>)</li><li data-line="112"><strong>Google’s <a data-tooltip-position="top" aria-label="https://pair.withgoogle.com/guidebook/" rel="noopener" class="external-link" href="https://pair.withgoogle.com/guidebook/" target="_blank">People + AI Guidebook</a></strong> is rooted in data and insights drawn from Google’s product team and academic research. (<a data-tooltip-position="top" aria-label="https://read.readwise.io/read/01ha0knv5k80y6rkxqsxa2t1he" rel="noopener" class="external-link" href="https://read.readwise.io/read/01ha0knv5k80y6rkxqsxa2t1he" target="_blank">View Highlight</a>)</li><li data-line="113"><strong>Apple’s <a data-tooltip-position="top" aria-label="https://developer.apple.com/design/human-interface-guidelines/machine-learning" rel="noopener" class="external-link" href="https://developer.apple.com/design/human-interface-guidelines/machine-learning" target="_blank">Human Interface Guidelines for Machine Learning</a></strong> differs from the bottom-up approach of academic literature and user studies. (<a data-tooltip-position="top" aria-label="https://read.readwise.io/read/01ha0kp7bzg6c4x2cjgvf1n0z6" rel="noopener" class="external-link" href="https://read.readwise.io/read/01ha0kp7bzg6c4x2cjgvf1n0z6" target="_blank">View Highlight</a>)</li><li data-line="114"><strong>Set the right expectations.</strong> This principle is consistent across all three guidelines: (<a data-tooltip-position="top" aria-label="https://read.readwise.io/read/01hdpgetrmrv698vfpejqngwn5" rel="noopener" class="external-link" href="https://read.readwise.io/read/01hdpgetrmrv698vfpejqngwn5" target="_blank">View Highlight</a>)</li><li data-line="115">Microsoft: Make clear how well the system can do what it can do (help the user understand how often the AI system may make mistakes) (<a data-tooltip-position="top" aria-label="https://read.readwise.io/read/01hdpgey9e53a39ygfm40h1cdb" rel="noopener" class="external-link" href="https://read.readwise.io/read/01hdpgey9e53a39ygfm40h1cdb" target="_blank">View Highlight</a>)</li><li data-line="116">Google: Set the right expectations (be transparent with your users about what your AI-powered product can and cannot do) (<a data-tooltip-position="top" aria-label="https://read.readwise.io/read/01hdpgf1f16895srnda2bekz0w" rel="noopener" class="external-link" href="https://read.readwise.io/read/01hdpgf1f16895srnda2bekz0w" target="_blank">View Highlight</a>)</li><li data-line="117">Apple: Help people establish realistic expectations (describe the limitation in marketing material or within the feature’s context) (<a data-tooltip-position="top" aria-label="https://read.readwise.io/read/01hdpgf4ax1ezd155rw2bqqsvv" rel="noopener" class="external-link" href="https://read.readwise.io/read/01hdpgf4ax1ezd155rw2bqqsvv" target="_blank">View Highlight</a>)</li><li data-line="118"><strong>Provide attribution.</strong> This is listed in all three guidelines: (<a data-tooltip-position="top" aria-label="https://read.readwise.io/read/01hdpgffpn8vvw6exd7328bjkg" rel="noopener" class="external-link" href="https://read.readwise.io/read/01hdpgffpn8vvw6exd7328bjkg" target="_blank">View Highlight</a>)</li><li data-line="119">Microsoft: Make clear why the system did what it did (enable the user to access an explanation of why the AI system behaved as it did) (<a data-tooltip-position="top" aria-label="https://read.readwise.io/read/01hdpgfmv8kcqx66h9epsms083" rel="noopener" class="external-link" href="https://read.readwise.io/read/01hdpgfmv8kcqx66h9epsms083" target="_blank">View Highlight</a>)</li><li data-line="120">Google: Add context from human sources (help users appraise your recommendations with input from 3rd-party sources) (<a data-tooltip-position="top" aria-label="https://read.readwise.io/read/01hdpgfpe254b6gtsbygsagcjz" rel="noopener" class="external-link" href="https://read.readwise.io/read/01hdpgfpe254b6gtsbygsagcjz" target="_blank">View Highlight</a>)</li><li data-line="121">• Apple: Consider using attributions to help people distinguish among results (<a data-tooltip-position="top" aria-label="https://read.readwise.io/read/01hdpgfqw2rn0ckpyjmr49spae" rel="noopener" class="external-link" href="https://read.readwise.io/read/01hdpgfqw2rn0ckpyjmr49spae" target="_blank">View Highlight</a>)</li><li data-line="122">In general, the more effort a user puts in (e.g., chat, search), the higher the expectations they have. Contrast this with lower-effort interactions such as scrolling over recommendations slates or clicking on a product. (<a data-tooltip-position="top" aria-label="https://read.readwise.io/read/01ha0kznxwjj6p51ep6ps0fvtd" rel="noopener" class="external-link" href="https://read.readwise.io/read/01ha0kznxwjj6p51ep6ps0fvtd" target="_blank">View Highlight</a>)</li><li data-line="123">Collect user feedback: To build our data flywheel (<a data-tooltip-position="top" aria-label="https://read.readwise.io/read/01ha0m22wnqj7qa3t61jfewpr7" rel="noopener" class="external-link" href="https://read.readwise.io/read/01ha0m22wnqj7qa3t61jfewpr7" target="_blank">View Highlight</a>)</li><li data-line="124">ChatGPT is one such example. Users can indicate thumbs up/down on responses, or choose to regenerate a response if it’s really bad or not helpful. This is useful feedback on human preferences which can then be used to fine-tune LLMs. (<a data-tooltip-position="top" aria-label="https://read.readwise.io/read/01ha0m53s372jr1vv207zg8v7c" rel="noopener" class="external-link" href="https://read.readwise.io/read/01ha0m53s372jr1vv207zg8v7c" target="_blank">View Highlight</a>)</li><li data-line="125">Midjourney is another good example. After images are generated, users can generate a new set of images (negative feedback), tweak an image by asking for a variation (positive feedback), or upscale and download the image (strong positive feedback). This enables Midjourney to gather rich comparison data on the outputs generated. (<a data-tooltip-position="top" aria-label="https://read.readwise.io/read/01ha0m50eg6ccpvt8x2vwnvc9w" rel="noopener" class="external-link" href="https://read.readwise.io/read/01ha0m50eg6ccpvt8x2vwnvc9w" target="_blank">View Highlight</a>)</li><li data-line="126">Also, how long is the average conversation? This can be tricky to interpret: Is a longer conversation better because the conversation was engaging and fruitful? Or is it worse because it took the user longer to get what they needed? (<a data-tooltip-position="top" aria-label="https://read.readwise.io/read/01ha0m6r0nadqpmab25j9zjjqw" rel="noopener" class="external-link" href="https://read.readwise.io/read/01ha0m6r0nadqpmab25j9zjjqw" target="_blank">View Highlight</a>)</li></ul></div></div></div><div class="heading-wrapper"><h2 data-heading="New highlights added 2023-11-16 at 3:35 PM" class="heading" id="New_highlights_added_2023-11-16_at_3:35_PM"><div class="heading-before"></div><div class="heading-collapse-indicator collapse-indicator collapse-icon"><svg xmlns="http://www.w3.org/2000/svg" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="svg-icon right-triangle" style="width:24px;max-width:100%"><path d="M3 8L12 17L21 8"></path></svg></div>New highlights added 2023-11-16 at 3:35 PM<div class="heading-after">...</div></h2><div class="heading-children"><div><ul><li data-line="0">Caching can significantly reduce latency for responses that have been served before. In addition, by eliminating the need to compute a response for the same input again and again, we can reduce the number of LLM requests and thus save cost. (<a data-tooltip-position="top" aria-label="https://read.readwise.io/read/01hfaa0m2vzvkbf8vj1pqjrsps" rel="noopener" class="external-link" href="https://read.readwise.io/read/01hfaa0m2vzvkbf8vj1pqjrsps" target="_blank">View Highlight</a>)</li></ul></div><div class="mod-footer"></div></div></div></div></div></div></div></div><div class="sidebar-right sidebar"><div class="sidebar-gutter"><div class="clickable-icon sidebar-collapse-icon"><svg xmlns="http://www.w3.org/2000/svg" width="100%" height="100%" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="3" stroke-linecap="round" stroke-linejoin="round" class="svg-icon"><path d="M21 3H3C1.89543 3 1 3.89543 1 5V19C1 20.1046 1.89543 21 3 21H21C22.1046 21 23 20.1046 23 19V5C23 3.89543 22.1046 3 21 3Z"></path><path d="M10 4V20"></path><path d="M4 7H7"></path><path d="M4 10H7"></path><path d="M4 13H7"></path></svg></div></div><div class="sidebar-container"><div class="sidebar-sizer"><div class="sidebar-content-positioner"><div class="sidebar-content"><div class="graph-view-wrapper"><div class="sidebar-section-header">Interactive Graph</div><div class="graph-view-placeholder"><div class="graph-view-container"><div class="graph-icon graph-expand" role="button" aria-label="Expand" data-tooltip-position="top"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="svg-icon"><line x1="7" y1="17" x2="17" y2="7"></line><polyline points="7 7 17 7 17 17"></polyline></svg></div><canvas id="graph-canvas" width="512px" height="512px"></canvas></div></div></div><div class="tree-container outline-tree" data-depth="0"><div class="tree-header"><span class="sidebar-section-header">Table Of Contents</span><button class="clickable-icon collapse-tree-button"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"></svg></button></div><div class="tree-scroll-area"><div class="tree-item" data-depth="1"><div class="tree-item-contents"><a class="internal-link tree-item-link" href="#Design_Patterns_for_LLM_Systems_&amp;_Products"><span class="tree-item-title"><p>Design Patterns for LLM Systems &amp; Products</p></span></a></div><div class="tree-item-children"><div class="tree-item" data-depth="2"><div class="tree-item-contents"><a class="internal-link tree-item-link" href="#Highlights"><span class="tree-item-title"><p>Highlights</p></span></a></div><div class="tree-item-children"></div></div><div class="tree-item" data-depth="2"><div class="tree-item-contents"><a class="internal-link tree-item-link" href="#New_highlights_added_2023-11-16_at_3:35_PM"><span class="tree-item-title"><p>New highlights added 2023-11-16 at 3:35 PM</p></span></a></div><div class="tree-item-children"></div></div></div></div></div></div></div></div></div></div></div></div></body></html>